[
{
	"uri": "https://tai-isme.github.io/workshop-template/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "Optimizing HPC Workflows with Auto-Scaling Clusters in Ansys Gateway Powered by AWS By Dnyanesh Digraskar and Trent Andrus | April 22, 2025 | in AWS ParallelCluster, High Performance Computing, Partner solutions.\nContributed by Dnyanesh Digraskar, Senior Partner Solutions Architect for HPC at AWS, and Trent Andrus, Product Expert at Ansys.\nAnsys Gateway powered by AWS is a cloud engineering solution (referred to as \u0026ldquo;Ansys Gateway\u0026rdquo; in the rest of this blog) hosted on AWS Marketplace. This product provides customers with a seamless interface to run Ansys simulations within their Amazon Web Services (AWS) accounts. Customers can quickly deploy authenticated, optimized Ansys applications on HPC infrastructure recommended by AWS. Groupama FDJ and Turntide Technologies use Ansys Gateway to accelerate engineering design and simulation workflows. Compared to on-premise infrastructure, Groupama FDJ achieved 17x faster simulation speeds when designing racing bicycles, while Turntide Technologies achieved 7x faster electric motor design simulations.\nFollowing the initial release of Ansys Gateway, user feedback indicated a need for greater control over the HPC cluster creation process. To address this, Ansys Gateway has integrated auto-scaling cluster capabilities starting from version 2024 R2. This enables dynamic resource allocation based on Slurm job queue demand. Beyond the enhanced cluster creation workflow, Ansys Gateway now supports AWS services such as Amazon FSx for Lustre, Amazon FSx for OpenZFS, and Amazon Elastic File System (EFS). These services are commonly used to support various HPC workflows. With auto-scaling capabilities, EFS is an excellent starting choice for most customers. For large-scale simulations with heavy I/O operations, high-performance shared file systems like FSx for Lustre and OpenZFS help avoid I/O bottlenecks. In mixed Windows and Linux computing environments, OpenZFS is accessible from both platforms.\nScaling HPC resources for engineering simulations is a complex challenge. Scalability reflects the HPC infrastructure\u0026rsquo;s ability to provide compute resources proportional to workload demand. Typically, tightly coupled simulations such as Ansys Fluent fluid simulation software, Ansys LS-DYNA nonlinear dynamics simulation software, and Ansys Mechanical structural finite element analysis software can scale across multiple cores and achieve near-linear performance (depending on benchmarks). With auto-scaling clusters, users can efficiently manage resources based on workload demand and eliminate hardware bottlenecks during peak periods.\nThis blog describes the architecture, workflow, and Amazon EC2 recommendations when running Ansys applications in Ansys Gateway.\nArchitecture Components The architecture components of Ansys Gateway, including the Control Plane and Application Plane, have been detailed in a blog from 2023.\nWith this release, Ansys Gateway integrates with AWS ParallelCluster, an open-source cluster management tool. Slurm is used as the workload orchestrator to automatically provision compute nodes when jobs are queued and deallocate them when no longer needed. Figure 1 below illustrates the architecture diagram of AWS ParallelCluster deployment in the Ansys Gateway customer\u0026rsquo;s VPC. The architecture diagram showing AWS ParallelCluster deployment in the customer\u0026rsquo;s Amazon Virtual Private Cloud (VPC) for Ansys Gateway is illustrated in Figure 1.\nFigure 1: Ansys Gateway HPC deployment architecture based on AWS ParallelCluster. This is the application layer deployed in the customer's AWS account.\nMain Components of the HPC Cluster Include: Head Node – Manages job workflows and cluster state. Slurm Controller – Workload orchestrator using Slurm workload manager. Dynamic Compute Nodes – Provisioned and terminated based on workload demand. For example: launching Amazon EC2 instance types such as C6i, Hpc6a, Hpc7a for CPU-based workloads and P5, G6e for GPU acceleration. Storage – Amazon EFS or Amazon Elastic Block Store (EBS) for application installation and persistent job data that can function as Network File System (NFS) mounts, or Amazon FSx for Lustre and Amazon FSx for OpenZFS for high-performance workloads. Cluster Queues – Job queues supporting multiple instance types for various workloads, such as: compute-optimized instance queues for running CFD and impact simulations; memory-optimized instance queues for running FEA or NVH simulations; GPU-optimized instance queues for graphics-oriented simulations. You should run applications on uniform instance types with all compute nodes having the same design for optimal performance. Advanced Networking – Amazon Elastic Fabric Adapter (EFA) is an advanced network interface for Amazon EC2 instances to run applications requiring high-level inter-node communication at large scale. AWS ParallelCluster brings significant flexibility to Ansys Gateway in defining compute resources and queues. A queue can contain multiple compute resources, and each resource can have multiple EC2 instance types. For example: a queue with compute resources containing instance types such as r6i.32xlarge, r6in.32xlarge, r6id.32xlarge. Typically, the cluster will attempt to provision the cheapest instance type first, and if insufficient, move to the next type. This approach helps temporarily address resource shortage issues.\nWith dynamic auto-scaling clusters, Ansys Gateway users can now employ job-based (queue-based) simulation workflows rather than the traditional approach of a dedicated cluster per simulation. With static clusters, resources may be wasted during off-peak times, leading to unnecessary costs. Static clusters may also not suit all job types, requiring you to resize or create new clusters with different instance types, causing downtime and unexpected expenses.\nSimulation Job Submission Workflow This section describes the step-by-step workflow performed on the Ansys Gateway interface to create dynamic auto-scaling HPC clusters for running Ansys simulations. It is assumed you have logged into Ansys Gateway and have access to workspaces to submit simulation jobs.\nStep 1: Create HPC Cluster Creating a dynamic auto-scaling cluster is performed as follows:\nSelect storage type (among Amazon EFS, Amazon FSx for OpenZFS, and Amazon FSx for Lustre) Select the Ansys application packages to install Define the cluster\u0026rsquo;s compute queues and select resources As with creating any resource, you start by selecting a tenant (i.e., a registered workspace) in Ansys Gateway as illustrated in Figure 2a. Then, create a new or select an existing project space that does not yet have a dynamic auto-scaling cluster. This project space must have the \u0026ldquo;24R2+\u0026rdquo; indicator in the Releases column as illustrated in Figure 2b.\nFigure 2a: Ansys Gateway home page. Users can select a tenant to access their project spaces. Two tenant options are displayed, each with its own name and ID.\nFigure 2b: Ansys Gateway project space home page. A project space titled \"Gateway Autoscaling Cluster Demonstration\" is displayed.\nWithin this project space, create a new resource and select \u0026ldquo;Autoscaling Cluster\u0026rdquo; from the dropdown list as illustrated in Figure 3.\nFigure 3: An empty project space with a dropdown list open, prompting the user to create a new virtual desktop or clusters.\nAfter selecting \u0026ldquo;Autoscaling Cluster\u0026rdquo; from the dropdown list, the new creation wizard will allow you to configure several key cluster features. First is the storage type, followed by the applications to install, and finally the compute resource queues. By default, an Amazon Elastic File System (EFS) drive will be mounted to the HPC cluster and available to resources within the project space. In Figure 4, a second optional storage location is defined for the product installation path.\nFigure 4: Storage options in the cluster creation wizard. By default, an EFS file system will be created and mounted to the cluster. Additionally, a second storage area, Amazon FSx for OpenZFS, is configured with 256 GiB capacity and 2048 MiB/s/TiB throughput. Storage name and mount path are provided by the user.\nOnce storage options are determined, you can select the application packages you wish to install. In Figure 5, Ansys Structures is selected.\nFigure 5: Step to select simulation applications to install in the cluster creation wizard.\nDuring cluster creation, the wizard can automatically deploy a server running Ansys HPC Platform Services (HPS), as illustrated in Figure 6. When using HPS, users can significantly simplify job submission through the ability to upload, submit, monitor, and download results directly from their workstation without manually transferring files to the cloud or writing job submission scripts. Note that HPS is provided as a container and uses Docker for deployment. For more information about setting up and using HPS, refer to the Ansys HPC Platform Services documentation.\nThe HPS installation location must match where the Ansys product is installed (for example: both installed on FSx for OpenZFS).\nFigure 6: Step to configure Ansys HPC Platform Services (HPS) in the cluster creation wizard. By default, a Linux-based virtual machine will be created for users and HPS services will be automatically deployed.\nNext, the wizard helps users create job submission queues. Users can specify both static and dynamic node quantities within their AWS service quota limits. Up to ten queues can be defined for a cluster.\nFigure 7: After clicking the \"Add queue\" button, users will select the application for this queue, name the queue, choose the number of static nodes (always available), maximum dynamic nodes, advanced options (such as EFA, placement group, etc.), and finally the instance type or types to use. This process is repeated for each queue.\nThe application associated with a queue is selected from the dropdown list. Typically, queue names should clearly reflect the application and version the queue uses, such as \u0026ldquo;mech242\u0026rdquo;.\nAfter defining the desired queues, you can name and create the cluster. When cluster creation completes, you will see a \u0026ldquo;Running\u0026rdquo; badge indicating that all resources and services have been deployed. Compute resources will remain offline until jobs are submitted. From the project space, clicking on the cluster displays the overview page with information about the head node, HPS node, queues, and provisioned compute nodes. See Figures 8a and 8b for details about the information displayed after cluster creation.\nFigure 8a: Overview page after cluster creation, displaying detailed information about the head node and HPS node.\nFigure 8b: Cluster overview page displaying: active queues by name (one queue named mech242), number of provisioned nodes (0/10), associated application (Ansys Structure 2024 R2); list of applications with installation locations (Ansys Structures on Amazon FSx for Open ZFS); list of mounted storage locations (default EFS, OpenZFS) with name, type, and mount path.\nStep 2: Submit Job to Cluster With the cluster created, you can take advantage of the simplified job submission workflow that HPS provides. For example, with Ansys Mechanical in Figure 9, connecting to the HPS server simply involves entering the HPS server\u0026rsquo;s IP address in the format https://example.com:port/hps. Refer to Figure 9 for the HPS server address.\nFigure 9: Solve Process Settings window in Ansys Mechanical.\nFor users familiar with Linux command line, you can still submit jobs using standard Slurm commands such as srun, sbatch, and salloc. Refer to Ansys Help documentation for more information about submitting jobs via Slurm.\nStep 3: Monitor Auto-Scaling Node Activity After submitting a job, users access the HPS job monitoring interface on the Ansys Gateway cluster page to track job status (Pending, Running, Evaluated), view log files, and download result files.\nFigure 10: HPS job monitoring interface. Users can access this interface from a web browser to view job status (Pending, Running, Evaluated), track log files, and download individual files.\nUsing Slurm Commands for Cluster Operations\nUsers familiar with monitoring via Slurm commands can still do so. Jobs submitted via HPS can be viewed through the \u0026ldquo;squeue\u0026rdquo; command in Figure 11 (after connecting to the Linux VDI in the workspace). The requested node is in configuring (CF) state, meaning it is being scaled up.\nFigure 11: Slurm query using the \"squeue\" command on the Linux command line. This command returns a list of jobs with information such as job status and allocated resources.\nSimilarly, you can view the number of provisioned nodes using the \u0026ldquo;sinfo -s\u0026rdquo; command as in Figure 12. A/I/O/T represents the number of Available/Idle/Offline/Total nodes.\nFigure 12: Slurm query using the \"sinfo\" command to view active queues. The result returns one queue with information about its name and number of nodes in use.\nUsers can also submit jobs using Slurm commands like \u0026ldquo;srun,\u0026rdquo; \u0026ldquo;sbatch,\u0026rdquo; and \u0026ldquo;salloc\u0026rdquo; as illustrated in Figure 13. Job submission scripts are pre-integrated in the application package installed on Ansys Gateway.\nFigure 13: Submit jobs directly to Slurm using the \"salloc\" command and track status using the \"squeue\" command.\nStep 4: Retrieve Results and Shutdown Cluster If submitting jobs via HPS, you can download result files directly to your personal computer from the cluster\u0026rsquo;s shared storage. If submitting via Slurm commands using temporary storage on instances, copy result files to shared storage when the job completes. By default, dynamic nodes will remain online for an additional 10 minutes after completing execution, then automatically shut down if no new jobs arrive.\nRecommended Amazon EC2 Instance Types for Ansys Applications Ansys Gateway supports advanced cluster creation workflows for the following applications:\nAnsys Electronics Desktop Ansys Fluids Ansys LS-DYNA Ansys Lumerical Ansys Pathfinder-SC Ansys Speos Ansys Structures Ansys Totem-SC Detailed setup workflows for each application are available in the Ansys Help documentation Recommended Configurations by Application, under the Recommended Usage Guide section. After learning the advanced process for creating HPC clusters on Ansys Gateway, refer to Table 1 for general recommendations on common Amazon EC2 instance types when running Ansys applications on Ansys Gateway. See the detailed list of recommended instance types at the Ansys Help page. Amazon EC2 Instances Specifications HPC6id HPC7a / HPC6a C6i* P5+ P4d+ G6e G5 Processor Intel Ice Lake AMD EPYC Intel Ice Lake NVIDIA H100 NVIDIA A100 L40S NVIDIA A10G Instance Size^ 32xlarge 96xlarge / 48xlarge 32xlarge 48xlarge 24xlarge 48xlarge 48xlarge Physical Cores 64 192 / 96 64 96 48 96 96 RAM per node (GiB) 1024 768 / 384 256 2048 1152 1536 768 Memory per core (GiB) 16 4 – 32 (HPC7a) / 4 (HPC6a) 4 24 24 16 8 EFA Bandwidth (GB/s) 200 300 / 100 50 3200 400 400 100 Number of GPUs 8 8 8 8 RAM per GPU (GB) 640 HBM3 40 HBM2 48 24 Target Ansys Apps^ Electronics Desktop, Fluids, LS-DYNA, Lumerical, Structures Fluids, LS-DYNA, Structures Electronics Desktop, Fluids, LS-DYNA, Lumerical, Pathfinder, Speos, Structures, Totem-SC Fluids Fluids Fluids, Structures, HFSS Fluids, Discovery Physics Description Implicit, Explicit, CFD codes Explicit, CFD codes Implicit, Explicit, CFD, Optics codes CFD codes Implicit, Explicit, CFD codes Implicit, Explicit, CFD codes Interactive modeling and simulations *Enable Elastic Fabric Adapter (EFA) to accelerate inter-node communication. Disable Simultaneous Multithreading (SMT) to ensure stable CPU performance. ^ HPC applications typically perform more efficiently when using the entire instance at maximum size, thanks to features like EFA. Cannot run directly on Ansys Gateway and requires users to configure additional NVIDIA packages. Table 1: Recommended Amazon EC2 instance types for various Ansys applications.\nConclusion Ansys Gateway powered by AWS has integrated with AWS ParallelCluster, enabling users to deploy on-demand HPC clusters to run Ansys simulations on AWS. This allows engineers to efficiently perform large-scale simulations while optimizing cloud computing costs. By automatically adjusting resources based on simulation workload demand, Ansys Gateway helps minimize idle compute time and ensure optimal scalability for HPC tasks.\nTo get started with Ansys Gateway, visit Ansys Gateway on AWS Marketplace to deploy an HPC environment on the cloud with just a few clicks. Experience the benefits of high-performance, flexible engineering simulations on AWS today.\nAbout the Authors﻿ Dnyanesh Digraskar Dnyanesh Digraskar is a Senior Partner HPC Solutions Architect at AWS. He leads HPC deployment strategy with AWS Independent Software Vendor (ISV) partners to help them build scalable and well-architected solutions. He has more than fifteen years of experience in CFD, CAE, numerical simulation, and HPC fields. Dnyanesh holds a Master\u0026rsquo;s degree in Mechanical Engineering from University of Massachusetts, Amherst. Trent Andrus Trent is a Product Expert at Ansys, focusing on topics related to HPC and cloud computing. With a passion for staying current with advances in hardware and software performance, he is dedicated to translating his knowledge into solutions that make HPC more accessible to all users. Outside of work, Trent is an avid cyclist and regularly trains to improve his own performance. "
},
{
	"uri": "https://tai-isme.github.io/workshop-template/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "Modernizing Snowflake Corporate\u0026rsquo;s Kubernetes Infrastructure with Bottlerocket and Karpenter By Sameeksha Garg, Gaurav Singodia, Jagdish Pawar, RK Sai (Ravikiran Koduri), and Sayan Moitra | April 18, 2025 | In Amazon Elastic Kubernetes Service, Customer Solutions, Open Source\nSnowflake Corporate IT Cloud Operations has reached a critical milestone in its cloud infrastructure development journey. Managing containerized workloads on Amazon Elastic Kubernetes Service (Amazon EKS) requires a modern, secure, and efficient operating system. The current system, running on Amazon Linux 2 (AL2), was performing well but presented certain challenges. Enhanced security required frequent updates, creating operational pressure. Ensuring safe and consistent updates across large numbers of nodes was difficult. Additionally, startup time for AL2 nodes was lengthy, resulting in poor scaling efficiency. After comprehensive evaluation, Bottlerocket, AWS\u0026rsquo;s container-optimized operating system, was selected as the ideal solution to address these challenges.\nMigration Strategy The transition from AL2 to Bottlerocket was not merely a technical change but a strategic decision to ensure Snowflake Corporate\u0026rsquo;s Kubernetes system is future-proof. Given the scale and complexity of workloads, the migration strategy was designed to ensure zero downtime, minimal disruption, and seamless auto-scaling. To achieve this, Snowflake Corporate adopted Karpenter – an open-source Kubernetes cluster autoscaler – along with NodePool and NodeClass to support dynamic node provisioning. The migration was executed in phases to minimize risks and ensure stability.\nMigration Steps The migration process began by preparing the cluster. Bottlerocket AMIs were integrated into the EKS environment by modifying NodePool and NodeClass configurations to use Bottlerocket as the default AMI family. AWS Identity and Access Management (IAM) policies were optimized to align with Bottlerocket\u0026rsquo;s security model, following the principle of least privilege.\nThis architecture diagram illustrates the migration strategy:\nKarpenter deployment replaced the traditional static resource provisioning approach, enabling nodes to be created on-demand. Workload validation was performed using a staging environment to test on Bottlerocket nodes before moving to production. Performance monitoring was implemented through Fluentd and Datadog to track real-time metrics, while security compliance checks ensured Bottlerocket\u0026rsquo;s immutable infrastructure aligned with Snowflake Corporate\u0026rsquo;s security policies.\nDeployment was executed in phases, starting with stateless applications. Node affinity, pod anti-affinity, and categories were used to ensure optimal workload distribution. Gradually introducing Bottlerocket nodes helped workloads transition smoothly alongside existing AL2 instances. The cordon and drain process supported phased removal of AL2 instances without service disruption.\nFinally, advanced monitoring and optimization solutions were deployed. Auto-scaling with Karpenter dynamically adjusted the cluster\u0026rsquo;s node pool. Performance tuning was performed based on actual workload patterns, while enhanced observability provided insights into system health, enabling proactive issue detection and resolution.\nExample of NodeClass definition and NodePool association:\napiVersion: karpenter.k8s.aws/v1alpha5 kind: NodeClass metadata: name: bottlerocket-nodeclass spec: amiFamily: Bottlerocket instanceProfile: \u0026#34;KarpenterNodeInstanceProfile\u0026#34; securityGroupSelector: aws-ids: [\u0026#34;sg-0123456789\u0026#34;] Example of defining a NodePool: apiVersion: karpenter.k8s.aws/v1alpha5 kind: NodePool metadata: name: bottlerocket-nodepool spec: template: spec: nodeClassRef: name: bottlerocket-nodeclass limits: resources: cpu: 1000 ttlSecondsAfterEmpty: 30 Example of applying node affinity to schedule workloads on Bottlerocket nodes: apiVersion: apps/v1 kind: Deployment metadata: name: bottlerocket-app spec: replicas: 3 selector: matchLabels: app: bottlerocket-app template: metadata: labels: app: bottlerocket-app spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: karpenter.k8s.aws/node-pool operator: In values: - bottlerocket-nodepool containers: - name: app image: my-app-image:latest Example of using pod anti-affinity to spread workloads across different nodes: apiVersion: apps/v1 kind: Deployment metadata: name: workload-deployment spec: replicas: 3 selector: matchLabels: app: critical-app template: metadata: labels: app: critical-app spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - critical-app topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; containers: - name: workload image: workload-image:latest Example of NodePool definition:\napiVersion: karpenter.k8s.aws/v1alpha5 kind: NodePool metadata: name: bottlerocket-nodepool spec: template: spec: nodeClassRef: name: bottlerocket-nodeclass limits: resources: cpu: 1000 ttlSecondsAfterEmpty: 30 Example of applying node affinity to schedule workloads on Bottlerocket nodes:\napiVersion: apps/v1 kind: Deployment metadata: name: bottlerocket-app spec: replicas: 3 selector: matchLabels: app: bottlerocket-app template: metadata: labels: app: bottlerocket-app spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: karpenter.k8s.aws/node-pool operator: In values: - bottlerocket-nodepool containers: - name: app image: my-app-image:latest Example of using pod anti-affinity to spread workloads across different nodes:\napiVersion: apps/v1 kind: Deployment metadata: name: workload-deployment spec: replicas: 3 selector: matchLabels: app: critical-app template: metadata: labels: app: critical-app spec: affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - critical-app topologyKey: \u0026#34;kubernetes.io/hostname\u0026#34; containers: - name: workload image: workload-image:latest Challenges \u0026amp; Solutions Although Bottlerocket provides many benefits, the migration presented certain challenges. Some workloads initially experienced compatibility issues due to Bottlerocket\u0026rsquo;s immutable file system. To address this, application images were modified to fully comply with container standards and apply read-only configurations where appropriate. Bottlerocket also required reconfiguring IAM roles to align with the new security model; this was resolved through implementing fine-grained access controls and integrating IAM with Karpenter. To minimize risk, workloads were migrated in phases, ensuring application performance remained stable before completely removing AL2 nodes.\nKey Benefits The migration has delivered outstanding improvements in security, performance, and operational efficiency. Security was enhanced through immutable nodes, preventing unauthorized changes and eliminating configuration drift. Vulnerability exposure was significantly reduced by eliminating package managers, shell access, and SSH, limiting attack surface. Automatic and atomic updates ensure nodes are always patched for security without requiring system downtime.\nNode startup time improved thanks to optimized initialization processes, allowing new nodes to join the cluster quickly and increasing auto-scaling efficiency, ensuring workloads are rescheduled faster. Operational efficiency improved through dynamic scaling with Karpenter, provisioning resources precisely when needed and avoiding excess provisioning. Regarding costs, Bottlerocket\u0026rsquo;s lightweight operating system combined with Karpenter\u0026rsquo;s intelligent provisioning significantly reduced resource expenses.\nPerformance: Bottlerocket vs. AL2 Bottlerocket consistently delivers faster node readiness speed. Initial benchmarks show Bottlerocket reduces node readiness time by approximately 5 seconds compared to AL2. Additionally, Bottlerocket\u0026rsquo;s native container image caching mechanism saves approximately 36 seconds per pod on a new node, while unscheduled pods start approximately 40 seconds faster compared to AL2.\nEnhanced Security: AL2 vs. Bottlerocket A direct comparison of security improvements demonstrates why Bottlerocket is the superior choice:\nKey Takeaways\nThe migration process provided valuable lessons. Security and operational efficiency go hand in hand; Bottlerocket\u0026rsquo;s immutable design strengthens security for Snowflake Corporate. Automation simplifies processes, with Karpenter supporting real-time scaling, eliminating the need for manual intervention. Phased migration reduces risk, with phase-by-phase deployment allowing configuration refinement without impacting production environments.\nConclusion: Broader Implications for Enterprises Operating Large-Scale EKS The successful migration of Snowflake Corporate\u0026rsquo;s Kubernetes infrastructure to Bottlerocket and Karpenter has established a new standard model that the industry can reference. Outstanding benefits such as enhanced security, reduced node initialization time, and optimized operations can be widely applied across enterprises managing large-scale Kubernetes deployments. In the future, continued advancement with AI-based workload orchestration, deeper integration with system observability tools, and exploration of serverless Kubernetes using Bottlerocket is possible. The adoption of Bottlerocket and Karpenter not only enhanced security for Snowflake Corporate but also improved performance through dynamic scaling capabilities, highlighting the power of modern cloud-native solutions in building high-performance, stable, and sustainable Kubernetes environments.\nAbout the Authors Sameeksha Garg Sameeksha Garg is a Technical Account Manager at AWS, dedicated to supporting and advancing the cloud journey for AWS\u0026rsquo;s global enterprise customers. She has more than 7 years of industry experience, including expertise in cloud security, cloud operations, cloud infrastructure management, and enterprise customer care. Sameeksha is particularly passionate about cloud security technologies and consistently strives to help customers secure their workloads on the cloud platform. Gaurav Singodia Gaurav Singodia is a senior technology leader at Snowflake with a distinguished track record of driving innovation and growth through entrepreneurial thinking. He currently leads a diverse global organization encompassing SRE (Site Reliability Engineering), Systems Engineering, Software Engineering, Data Infrastructure, Identity Platform, AI/ML, and Analytics teams, with significant focus on maintaining high quality and achieving scalability across all domains. Jagdish Pawar Jagdish Pawar has more than 18 years of leadership experience in technology startups, growing enterprises, and large corporations. He has expertise in building and leading cross-functional teams, product management, engineering, and managing cloud operations with high reliability, security, and large-scale scalability. RK Sai (Ravikiran Koduri) RK Sai (Ravikiran Koduri) is Head of Enterprise Support at AWS. In his capacity as a technical advisor, he assists Independent Software Vendors (ISVs) in deploying large-scale workload operations. RK Sai is passionate about AWS Deep Racer, AI services, and Cloud Financial Management. In his spare time, he continuously strives to translate abstract concepts of fulfillment into concrete realities in life. Sayan Moitra Sayan Moitra is a Senior DevOps Engineer specializing in cloud engineering, DevOps, and Site Reliability Engineering (SRE), with expertise in infrastructure and application deployment. He holds multiple AWS and CKAD certifications and is recognized for his expertise in serverless computing. Sayan is passionate about continuous learning and solving complex problems. "
},
{
	"uri": "https://tai-isme.github.io/workshop-template/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Seamless Migration: Secure Transition of Large IoT Device Fleets to AWS By Andrea Sichel and Katja-Maja Kroedel | April 15, 2025 | In the topics: Advanced (300), AWS IoT Core, AWS IoT Device Management, Best Practices, Intermediate (200), Internet of Things, Migration, Technical How-to\nMigrating large-scale IoT device fleets to the cloud is one of the most complex technical transformations that organizations face today. While the benefits of cloud migration are clear, the path to successful deployment requires careful planning and execution. In a previous article, we analyzed the main reasons to migrate to AWS IoT Core. This article will share a proven strategy for transitioning IoT device fleets with hundreds of millions of devices to AWS IoT Core, address common challenges, outline a specific migration scenario, and dive into AWS IoT Core features that support complex migration use cases.\nChallenges with Self-Managed IoT Message Brokers Many organizations begin their IoT journey with self-managed message brokers. While this approach offers initial control and flexibility, it often becomes increasingly challenging as the number of devices grows. Understanding these challenges is critical before beginning the cloud migration process.\nHigh Costs The financial impact of maintaining and operating self-managed IoT infrastructure far exceeds basic storage costs. Organizations often struggle with inefficient capacity planning, requiring dedicated engineering teams to manage infrastructure. These teams must constantly balance priorities across different departments while maintaining system reliability. Costs for monitoring, security, and compliance further complicate the financial situation.\nMatching Compute Capacity One of the most difficult aspects of managing IoT infrastructure is aligning compute resources with workload demand. Peak usage periods require excess capacity to maintain performance, while low-usage periods result in wasted resource allocation. This challenge is particularly severe with global deployments, where usage patterns vary by region and time zone. Organizations often must choose between over-provisioning to ensure reliability or risking performance issues when unexpected spikes occur.\nUnresolved Security Challenges Security is perhaps the most critical challenge in large-scale IoT deployments. Managing millions of connected devices requires sophisticated security protocols, including certificate management, real-time threat detection, secure update mechanisms, and data transmission security. As regulatory requirements evolve, organizations must continuously update security practices while maintaining uninterrupted service. This requirement becomes increasingly complex as device fleets expand and distribute geographically.\nSlowed Innovation Perhaps the largest hidden cost from self-managed brokers is the impact on innovation. Technical teams spend excessive time maintaining existing infrastructure instead of developing new features or improving customer experience. This maintenance burden leads to slower product launches and missed market opportunities, affecting the organization\u0026rsquo;s competitive position.\nCustomer Scenario and Requirements Let\u0026rsquo;s consider a migration scenario that illustrates how complex IoT environments can successfully transition to AWS IoT Core.\nFigure 1: Customer scenario before migration\nArchitecture Imagine a customer with the following setup, illustrated in Figure 1:\n10 million devices: Connecting daily from multiple locations worldwide.\nOn-premise solution: Devices initially connect to on-premise brokers and backend services, executing logic for internal applications or support.\nDNS server: Used to connect to the self-managed MQTT broker.\nOver 80 backend services: Distributed microservices architecture with 20–100 instances per service.\nAPI Gateway: Applications consuming data interact with the backend through an API gateway.\nTechnical Requirements for the New Solution The new solution must meet strict technical requirements to ensure seamless transition:\nNon-intrusive device updates: The device fleet must transition without firmware changes or manual intervention, since field updates cannot be performed during the expected migration period.\nProtocol compatibility: Seamlessly support both MQTT3 and MQTT5 protocols, as the device fleet comprises multiple hardware generations using different protocol versions.\nAdvanced message distribution: Backend services need shared subscription functionality to efficiently balance loads and ensure consistent message handling across service instances.\nAWS IoT Core Features for Complex Migrations AWS IoT Core provides a feature set specifically designed to support complex migration use cases like the one illustrated above.\nAWS IoT Core operates on a shared responsibility model, defining security and operational boundaries. AWS manages and secures the underlying infrastructure, including physical data centers, service maintenance, and service availability. Customers are responsible for application security, device-level security implementation, certificate management, and developing business logic on AWS IoT Core.\nFigure 2: Core Features of AWS IoT\nHere are some key capabilities (services highlighted in bold are particularly well-suited to the customer\u0026rsquo;s architecture):\nIdentity service: Advanced device authentication using X.509 certificates, support for custom Certificate Authorities (CAs), and fine-grained access control through AWS IoT policies.\nDevice gateway: Flexible, scalable connectivity capable of supporting millions of concurrent connections, multi-protocol support (HTTPS, MQTT, MQTT over WebSockets, and LoRaWAN), with automatic load balancing.\nMessage broker: Low-latency message distribution with MQTT 3.1.1 and MQTT 5 support, shared subscriptions, and message persistence features.\nRegistry: Comprehensive device catalog with flexible metadata management, dynamic device grouping, integration with AWS IoT Device Management.\nKey Features for Complex Migrations AWS IoT Core provides a powerful set of features that simplify complex IoT device fleet migration processes, while addressing common challenges when upgrading to AWS-managed AWS IoT Core solutions. A crucial aspect of phased migration approaches is that these techniques enable backend services and devices to migrate at their own pace, thereby maximizing minimization of downtime and service interruptions. Let\u0026rsquo;s explore some important capabilities directly related to the customer migration scenario mentioned earlier:\nCustom domain: This feature stands out as a critical enabler for large-scale migration. It eliminates one of the biggest migration barriers by allowing organizations to use their current domain with AWS IoT Core endpoints. This means devices can continue operating with their existing configuration, significantly reducing migration risk and complexity. Additionally, users can configure policies and TLS versions as well as protocols and connection ports for the endpoint used.\nMQTT Support (MQTT 3 and MQTT 5): In diverse IoT systems, devices often use different MQTT versions. AWS IoT Core supports both MQTT 3.1.1 and MQTT 5, allowing devices using different MQTT versions to connect, ensuring smooth migration without requiring simultaneous upgrades of all devices to the latest MQTT standard.\nBring your own certificate authority (CA): Maintaining existing security infrastructure is critical during migration. AWS IoT Core allows you to register your current CA on AWS, establishing a chain of trust between devices and AWS IoT Core without needing to reissue certificates to devices. This eliminates the need for certificate rotation throughout the migration process.\nRecently, AWS IoT Core added features that enhance migration processes and improve performance:\nMessage enrichment with registry metadata: Propagates device attributes stored in the registry with each message sent, helping eliminate the need for AWS Lambda or processing instances to retrieve this information from other sources.\nThing-to-connection association: A \u0026ldquo;thing\u0026rdquo; is a registry entry containing attributes describing a device. Policies define the actions a device can perform on AWS IoT. The new feature allows using variables in policies with any client ID format, solving the migration challenge when client IDs don\u0026rsquo;t comply with AWS IoT Core \u0026ldquo;thing\u0026rdquo; naming rules. Once configured, multiple client IDs can be used for each certificate and \u0026ldquo;thing\u0026rdquo;, providing flexibility without needing to change device identifiers or existing configurations.\nClient ID in just-in-time registration (JITR): Performs additional security authentication during JITR by receiving client ID information.\nCustom client certificate validation: Allows custom validation of client certificates through AWS Lambda functions when devices connect. This supports integration with external authentication services like OCSP (Online Certificate Status Protocol), enhancing security controls throughout the device authentication process.\nCustom authentication with X.509 client certificates: Extends certificate authentication with Lambda, allowing dynamic policy assignment for devices upon connection. This feature complements the existing Custom Authorizer functionality that already supports JWT tokens and username/password.\nALPN TLS extension removal: The TLS ALPN extension is no longer required during security handshakes (Transport Layer Security handshake), eliminating barriers for devices that don\u0026rsquo;t support ALPN.\nThese features provide greater flexibility, security, and efficiency when managing IoT device fleets on AWS IoT Core. By leveraging them, organizations can minimize complexity and risk when migrating large device fleets, ensuring seamless transition to a modern, secure, and scalable cloud IoT platform.\nTarget Architecture The target architecture involves migrating 10 million devices connecting to AWS IoT Core through Amazon Route 53 (or any DNS server). Backend services, API gateway, and consuming applications remain unchanged.\nFigure 3: Target Architecture\nMigration Strategy The idea is to build a migration strategy based on five main pillars to ensure smooth transition. This process begins by maintaining a risk-free approach through careful planning and testing, while controlling operations with comprehensive documentation and monitoring. The strategy emphasizes minimizing errors through precise implementation steps and validation.\nAligned with the strategic principles outlined, a phased approach should be applied. Each phase has specific objectives and dependencies, allowing for close progress tracking and flexible adjustment when needed.\nLet\u0026rsquo;s dive into each phase, clarifying the rationale behind decisions and illustrating real-world implementation.\nPhase 0: Preparation The preparation phase lays the foundation for successful migration. The focus is establishing a bridge between the current infrastructure and AWS IoT Core, ensuring continuous operation throughout the process.\nCentral to this phase is deploying a republish layer (republish layer). This component serves as an intermediary, supporting bidirectional communication between the self-managed broker and AWS IoT Core — like a secure tunnel allowing messages to flow seamlessly between the two systems.\nFigure 4: Architecture of the Preparation Phase\nThe republish layer comprises two main components: Device to backend: This component collects messages from devices connected to your self-managed broker and forwards them to AWS IoT Core. Deploying this flow first enables beginning the process of transitioning backend services, while devices continue connecting to the existing broker.\nBackend to device: Deployed in parallel, this component ensures messages from newly transitioned backend services still reach devices still connected to the self-managed broker. This bidirectional communication capability maintains system integrity throughout the transition.\nWe recommend implementing the republish layer using container services like Amazon Elastic Container Service (ECS) or other suitable compute solutions. The source code for this component is straightforward: subscribe to a topic on one broker, then publish to the other. Container deployment provides flexibility to scale up/down as needed during migration.\nPhase 1: Backend Migration This phase focuses on migrating backend services from the self-managed broker to AWS IoT Core. Let\u0026rsquo;s see how to leverage the republish layer to migrate backends gradually without losing messages.\nDevice-to-backend republish layer During backend migration, maintaining even message distribution across shared subscriptions is critical to avoid subscriber overload. The republish layer integrates smoothly with existing instances, using shared subscription models to ensure balanced message distribution. As messages pass through this layer to migrated instances, carefully monitor each component to prevent overload. This careful approach enables gradual migration, preserving message distribution patterns and system stability.\nBackend-to-device republish layer The backend-to-device layer is deployed on Amazon\u0026rsquo;s ECS cluster, connecting to AWS IoT Core to consume messages. Unlike the device-to-backend layer, all backend-to-device instances can be deployed simultaneously since each instance handles specific device topics without overload concerns. This enables faster backend migration while ensuring messages reach devices.\nFigure 5: Architecture visualizing the device-to-backend republish layer for Service A migration process\nDuring backend migration, we recommend setting up AWS IoT Core rules to store messages to Amazon S3 as an important backup layer. Backup storage enables recovery and reprocessing if unexpected issues occur, ensuring no data loss from devices.\nWith the republish layer operating stably, the phased transition system proceeds as follows:\nIntroduce the first device-to-backend instance\nVerify message flow through this instance to AWS IoT Core and back to devices\nRemove the corresponding unmigrated backend instance\nContinue progressively with all backend instances\nThis approach enables smooth transition of all backend services to AWS IoT Core. The same strategy applies to other platform services, ensuring continuous operation throughout the entire process.\nFigure 6: Architecture visualizing the completed backend migration process to AWS IoT\nPhase 2: Device Migration This phase requires particular attention to detail as it directly impacts user experience and device connectivity.\nThe key to successful device migration is a weighted DNS routing strategy (or alternative strategy), for example using Amazon Route 53 (or any DNS). This approach enables fine-grained control over the transition process:\nBegin with a small traffic ratio (typically 1–2%) shifting to AWS IoT Core.\nMonitor device connections, message handoff, throttling threshold surpass capability, and error rates using AWS IoT metrics and dimensions in Amazon CloudWatch.\nGradually increase the ratio based on performance indicators.\nMaintain the ability to quickly reverse traffic if needed.\nThis phase leverages AWS IoT Core\u0026rsquo;s just-in-time device registration capability, automatically provisioning device connection resources. This automation minimizes operational overhead during large-scale migration.\nFigure 7: Architecture visualizing the device migration process\nAfter completing device migration, the republish layer continues operating, continuing to forward messages to the self-managed broker. This design provides an essential failback mechanism — if issues occur, traffic can be immediately reversed to the old broker while maintaining message communication between devices and backends.\nPhase 3: Cleanup The cleanup phase completes the migration process. The republish layer is removed first, disconnecting the self-managed broker. After monitoring systems and supporting components confirm no more traffic to the old broker, and the entire system operates stably through AWS IoT Core, proceed with decommissioning the broker — completing the migration.\nFigure 8: Architecture visualizing the completed migration process aligned with the target architecture\nThis controlled sequence ensures the transition occurs smoothly while maintaining system stability throughout the final transition phase.\nConclusion Organizations can successfully migrate large IoT device fleets to AWS IoT Core using a phased approach and adhering to the five-pillar strategy. This model minimizes risk, providing failback mechanisms at every phase. The preparation, backend migration, device migration, and cleanup processes ensure controlled, secure transition, allowing both backends and devices to migrate at their own pace, maintaining operational stability.\nFor detailed analysis and interactive visualization of the migration journey, watch the tutorial video series on the AWS IoT YouTube channel: Part 1 and Part 2. These videos provide additional perspectives and real-world illustrations of the concepts in this article. For more customer and partner cases that have migrated to AWS IoT, see this blog.\nRemember, successful IoT transformation is not just about migrating systems — it\u0026rsquo;s about building a foundation for future growth while ensuring business continuity throughout the transition process.\nAbout the Authors Andrea Sichel Andrea Sichel is an IoT Solutions Architect at Amazon Web Services, where he supports customers on their cloud application journey in the IoT domain. With curiosity and a customer-first philosophy, he develops innovative solutions and stays current with the latest technologies. Andrea enjoys tackling complex problems and helping organizations expand their vision of IoT transformation. Outside of work, Andrea coaches his son\u0026rsquo;s soccer team and is passionate about photography. When not on the field or behind the camera, he enjoys swimming to maintain balance between work and life. Katja-Maja Kroedel Katja-Maja Kroedel is an Advocacy Specialist for Databases and IoT at AWS. She helps customers maximize the potential of cloud technology. With a background in computer engineering and extensive IoT \u0026amp; databases experience, she collaborates closely with customers on cloud adoption guidance, strategy, and system migration in these domains. Katja loves innovative technology, building and experimenting with services like AWS IoT Core and AWS RDS. "
},
{
	"uri": "https://tai-isme.github.io/workshop-template/3-blogstranslated/3.4-blog4/",
	"title": "Blog 4",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://tai-isme.github.io/workshop-template/3-blogstranslated/3.5-blog5/",
	"title": "Blog 5",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://tai-isme.github.io/workshop-template/3-blogstranslated/3.6-blog6/",
	"title": "Blog 6",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://tai-isme.github.io/workshop-template/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Recap: DX Talk #7 — Reinventing DevSecOps with AWS Generative AI Event Purpose Sharing the impact of AI on system operations and real-world case studies.\nSpeakers Lê Thanh Đức — Cloud Delivery Manager, CMC Global Dư Quốc Thành — Technical Leader, CMC Global Văn Hoàng Kha — Cloud Engineer, AWS Community Builder Key Content 1. The Context of DevSecOps and the Impact of AI Why DevSecOps: Many enterprises are shifting to the Agile DevSecOps model to shorten release times, increase automation, and accelerate time-to-market. As release velocity increases, security risks also rise. For example: a commit might contain secrets; a dependency or image might have un-scanned vulnerabilities. Previously, security was often the responsibility of the Security team (after Dev and Ops finished their work), leading to difficult changes and high remediation costs (potentially 10–20 times higher). Therefore, security needs to be integrated throughout the process, from start to finish. Beyond adding tools and frameworks to the pipeline, every team member must share responsibility for security (Dev understands risks when coding; Ops understands safe deployment processes; Security understands how DevOps operates). 2. Framework for DevOps LifeCycle The framework is divided into 3 main layers:\nEarly Stage (Shift Left): Integrate security as early as possible.\nExample: SAST to scan source code, Software Composition Analysis to check dependencies, script scanning to detect API keys or leaked credentials in the code. Middle Stage: Apply DevSecOps during Build and Deploy, focusing on safety controls within the deployment environment.\nExample: Using DAST tools to check for vulnerabilities in images. Final Stage: Runtime Security and Continuous Feedback.\n3. The 7 Main Phases in DevOps LifeCycle (Real-world Case Studies) Phase 1: PLAN\nDefine security requirements right from the product planning stage. Risk analysis, Threat Modeling, and setting compliance goals aligned with project regulations/standards. The Dev — Sec — Ops triad needs to align on goals, processes, and resources to ensure Security is tied to Business from the start. Clearly define the Security Roadmap. Phase 2: CODE\nSecurity is integrated directly into the coding environment. Developers must adhere to secure coding standards and code reviews. Use SAST to detect errors during the coding process. Cultivate a \u0026ldquo;Security-First\u0026rdquo; mindset for developers. Phase 3: BUILD\nFully automate the CI/CD chain. Tools: Dependency scan, config validation to ensure builds do not contain malicious code or vulnerabilities. Ensure stable, reusable builds and software integrity before release. Phase 4: TEST\nPerform comprehensive security testing. Run vulnerability scans, DAST, penetration tests, audits. Update test cases according to detected vulnerabilities. Phase 5: DEPLOY\nCheck configurations \u0026amp; IaC, policy-as-code before deploying. Ensure the runtime environment complies with agreed security standards. Reduce manual errors, ensure safe deployment. Phase 6: OPERATE\nAutomate patching and continuous security updates. Maintain stability and safety post-release. Phase 7: MONITOR\nContinuous monitoring. Use Realtime Analytics and alerting tools for threats. DevSecOps helps strictly prevent rather than react, keeping software in a safe state. 4. Tools Supporting DevSecOps Pre-commit \u0026amp; Code Quality: SonarQube, Codacy, Semgrep (SAST), Gitleaks — scan code, check code coverage, check for secrets. Dependency \u0026amp; SBOM Scanning: Syft, Grype, Dependency-Track — package management and library vulnerability scanning. IaC \u0026amp; Policy-as-Code: Checkov, Tfsec — scan Terraform/Kubernetes config; OPA Gatekeeper, Kyverno — enforce policy \u0026amp; compliance automatically. SAST/DAST \u0026amp; Security Tests: Trivy, Checkmarx, Semgrep, Codacy — detect vulnerabilities in code and runtime. CI/CD Integration: Jenkins, GitHub Actions, GitLab CI, ArgoCD — automate safe build, test, and deploy. Monitoring \u0026amp; Logging: Prometheus, Grafana, Loki, Promtail — system monitoring and observability in real-time. Alerting \u0026amp; Governance: Slack webhook, email alerts, anomaly detection — rapid alert and response; centralized risk report — centralized risk analysis and reporting. 5. Real-world Case Studies of Gen-AI in DevSecOps Mr. Thanh\u0026rsquo;s Pipeline Demo: CI/CD flowchart (summary) — (Singapore Blockchain Project — Mr. Thanh)\nMr. Duc\u0026rsquo;s Pipeline Demo:\nPull source code from Bitbucket — Build via MSBuild — SonarQube analysis — Check SonarQube Quality Gate — Archive files — Push artifacts.\nDemo CI/CD pipeline (Philippines Client Project — Mr. Duc)\nFrom DevOps to AIOps? =\u0026gt; Current DevOps toolchains are integrating Gen-AI internally.\nImpact of AI on DevOps:\nAI helps detect and analyze vulnerabilities more intelligently. AI supports automated incident response and remediation. AI helps shorten time and reduce the load for the security team. AI helps DevSecOps continuously learn and improve over time. Benefits of Gen-AI in DevOps:\nAutomate and accelerate DevSecOps processes. Enhance proactive security. Optimize observability and incident response. Demo: Using Amazon Q to scan for vulnerabilities in code files; using Amazon Q to test Terraform MCP.\nQ/A — Good Questions and Answers How can AI/ML optimize AWS costs (cost optimization)?\n=\u0026gt; Use estimation tools provided by AWS to select the right resources. Auto Scaling policies can be integrated based on predictions, or CloudWatch logs can be used to trigger shutdowns based on timeframes or thresholds when not in use. Useful services: AWS Compute Optimizer, Cost Explorer, etc.\nDoes AI automatically fix security errors? If so, which part of the DevSecOps pipeline should humans focus on?\n=\u0026gt; Yes. AI can automatically fix certain security errors, but AI should only be viewed as a support tool. Every part of the pipeline still requires close human supervision, no matter how powerful the AI is.\n"
},
{
	"uri": "https://tai-isme.github.io/workshop-template/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Nguyen Hong Hieu Tai\nPhone Number: 0707333797\nEmail: tainhhse182011@fpt.edu.vn\nUniversity: FPT\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://tai-isme.github.io/workshop-template/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Workshop Introduction This workshop guides you through building a serverless Excel Import system on AWS with authentication and data processing features.\nObjectives By the end of this workshop, we will:\nGet familiar with serverless architecture: How to design applications with Lambda, API Gateway, S3, and DynamoDB Get familiar with event-driven architecture: Use S3 Event Notifications to trigger automatic processing Implement authentication: Integrate Amazon Cognito for authentication and API security Parse Excel in Lambda: Use the Apache POI library to read and process .xlsx/.xls files Deploy with AWS SAM: Get familiar with the SAM CLI to build and deploy, and apply IaC for the entire infrastructure. Key Components 8 Lambda Functions: Register, Confirm, Login, Logout, GenerateUploadUrl, ListImportJobs, GetJobStatus, ImportS3Trigger 3 DynamoDB Tables: Students, Courses, ImportJobs 1 S3 Bucket: Store Excel files with a lifecycle policy (auto-delete after 7 days) 1 Cognito User Pool: Manage users and authentication 1 API Gateway: REST API with a Cognito authorizer Time \u0026amp; Cost Estimated completion time: ~30 minutes\nCost: Free (All within the Free Tier)\nTo avoid unexpected charges, perform cleanup immediately after finishing the workshop.\nRequirements Basic understanding of AWS (console, regions, basic services) Ability to use the terminal/command line Ability to read Java "
},
{
	"uri": "https://tai-isme.github.io/workshop-template/5-workshop/5.4-deploy-backend/5.4.1-sam-build/",
	"title": "SAM Build",
	"tags": [],
	"description": "",
	"content": "Build Project with Maven Maven will compile the Java code and download all necessary dependencies.\nClean\ncd excel-import-workshop mvn clean Package application\nmvn package This process will:\nDownload dependencies Compile Java source code Package into a JAR file Build with AWS SAM The SAM CLI will prepare Lambda deployment packages.\nsam build This process will:\nRead template.yaml Find all Lambda functions Copy compiled code from target/ into .aws-sam/build/ Create deployment packages for each function "
},
{
	"uri": "https://tai-isme.github.io/workshop-template/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Network and find team members. Learn about AWS services. Create the first AWS account, get familiar with the console, and manage costs. Learn about support services and how to submit support requests. Learn about access management and how to set it up. Learn about VPC, design, deploy, and manage it. Learn about EC2 and deploy a Node.js application on EC2. Tasks to be completed this week: Day Tasks Start Date End Date Resources 2 - Get to know FCJ team members and form a group - Remember office rules, regulations, and training guidelines that have been announced 08/08/2025 11/08/2025 3 - Learn about AWS services + Compute + Storage + Networking \u0026amp; Content Delivery + Database + Analysis + Management Tools + Developer Tools + Machine Learning 09/08/2025 09/08/2025 https://cloudjourney.awsstudygroup.com/vi/ 4 - Create AWS Free Tier account - Learn AWS Console\n- Learn about AWS support service\n- Hands-on: + Create AWS account (Set up MFA for account) + Get familiar with console operations (Set default region, use service search bar, bookmark frequently used services, how to use widgets) 09/08/2025 09/08/2025 https://000001.awsstudygroup.com/vi/ 5 - Learn about access management with AWS IAM - Hands-on: + Create IAM Group, IAM Role, IAM User + Switch IAM Role 10/08/2025 10/08/2025 https://000002.awsstudygroup.com/vi/ 6 - Learn about VPC - Hands-on: + Learn how to design, deploy, and manage environments on AWS + Set up Site-to-site VPN connection - Learn about EC2 - Hands-on: + Launch instances + Deploy applications on the instances created 11/08/2025 12/08/2025 https://000003.awsstudygroup.com/vi/ 7 - Learn about EC2 - Hands-on: + Launch instances + Deploy applications on the instances created 13/08/2025 14/08/2025 https://000004.awsstudygroup.com/vi/ Week 1 Results: Understand what AWS is and master the basic service groups:\nCompute Storage Networking \u0026amp; Content Delivery Database Analysis Management Tools Developer Tools Machine Learning Successfully created and configured AWS Free Tier account.\nLearned about 4 AWS Support plans and the services of each plan.\nGot familiar with AWS Management Console and know how to find, access, and use services from the web interface.\nUnderstand what IAM is: IAM is used to identify and authorize who or which service can access what and how on AWS resources\nDistinguish the main components in IAM:\nIAM User: A user who logs into AWS Console to perform daily management tasks IAM Group: Represents a group of users in the system Used to divide permissions by role of project or department\u0026hellip; This group cannot contain other groups One user can belong to multiple groups or no group at all IAM Role: Used to assign temporary access rights to an entity that can interact with other resources (usually attached to EC2, Lambda, \u0026hellip;) Some resources cannot interact with other resources on AWS unless assigned a role (the role contains policies that determine what this resource is allowed to do)\nIAM Policies: Determine what users, groups, roles can or cannot do (written as JSON text) Inline Policy: Policy created and assigned directly within the entity Managed Policy: Policy created separately, can be assigned to multiple entities IAM Switch Role: The entity will temporarily drop its current permissions and use the permissions of the entity it switched to Understand what VPC is, its architecture and main components:\nSubnet: A subnetwork in VPC, divided into Public - Private Subnet Route Table: Routing table that directs network traffic (each subnet must be associated with a Route Table) (EC2 can only access internet with it) Internet Gateway: Allows Public Subnets to access the internet NAT Gateway: Allows Private Subnets to connect to the internet but not to be connected to Know how to create a VPC, create Subnets, Internet Gateway, Route Table, create Security Group\nDeploy EC2 instances in the Subnets created previously and perform connections\nCreate NAT Gateway to connect EC2 instances created in private Subnets to the internet\nKnow how to set up CloudWatch and Alert on VPC metrics\nSuccessfully created VPN environment with VPC and EC2\nSet up Site to Site VPN connection between two VPCs through Virtual Private Gateway and Customer Gateway\nLaunch Windows instances and Linux instances and connect successfully\nInstall LAMP and XAMPP to deploy AWS FCJ Management application on Windows instances and Linux instances\n"
},
{
	"uri": "https://tai-isme.github.io/workshop-template/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Know how to deploy and manage databases on RDS Know how to grant permissions to EC2 applications and understand which method to use Learn about S3 object storage service Learn about Lightsail and build applications on it Get familiar with Amazon CloudWatch interface and how to use it to manage metric provisioning Tasks to be completed this week: Day Tasks Start Date End Date Resources 2 - Deploy backend system using Amazon EC2, Amazon RDS and Node.js - Hands-on: + Create VPC, subnet, security group for EC2 and RDS + Create EC2 instance, SSH using MobaXterm + Install Git, Node.js, MySQL client on EC2 + Clone source code from GitHub + Install npm packages, configure .env to connect RDS + Perform RDS backup, restore, monitoring 15/09/2025 15/09/2025 https://000005.awsstudygroup.com/vi/ 3 - Grant permissions to applications to access AWS services using IAM Role - Hands-on: + Create EC2, S3 bucket, IAM User with Access Key + Upload files to S3 using access key (not secure) + Create IAM Role assigned to EC2 to upload to S3 (secure) - Use AWS Cloud9 for programming - Hands-on: + Create Cloud9 instance + Use terminal, create file, edit text, run AWS CLI 16/09/2025 16/09/2025 https://000048.awsstudygroup.com/vi/ https://000049.awsstudygroup.com/vi/ 4 - Deploy static website on Amazon S3 and accelerate with CloudFront - Hands-on: + Create S3 bucket and upload website data + Enable Static Website Hosting and configure index.html + Configure Block Public Access and set public access for necessary objects + Check website via S3 endpoint + Integrate CloudFront to accelerate and keep bucket private + Enable S3 Versioning, practice changing index.html file and restore old version + Check website content via CloudFront 17/09/2025 17/09/2025 https://000057.awsstudygroup.com/vi/ 5 - Deploy open-source applications on Amazon Lightsail - Hands-on: + Create Lightsail Database instance + Deploy 3 applications: WordPress, PrestaShop and Akaunting + Configure Networking (assign Static IP to each instance) + Connect Lightsail database + Configure domain and Apache for Akaunting + Create separate MySQL databases for each application + Configure admin accounts in each application + Configure security: · Disable SSH (Port 22) · Set up Firewall Rules - Deploy containers with Amazon Lightsail Containers - Hands-on: + Create Lightsail Container + Deploy container from public Nginx image + Create Lightsail Container + Create Lightsail Instance, install Docker and AWS CLI + Build container image, push to Lightsail, deploy container image 18/09/2025 18/09/2025 https://000045.awsstudygroup.com/vi/ https://000046.awsstudygroup.com/vi/ 6 - Get familiar with CloudWatch Dashboards Hands-on: + Use CloudFormation to create EC2 instances with sample applications to generate metrics and logs + View, filter, calculate and display metrics from applications on EC2 + Observe logs, create log groups/streams, metric filters + Create alarms based on metrics and receive notifications via SNS 19/09/2025 19/09/2025 https://000008.awsstudygroup.com/vi/ Week 2 Results: Know how to set up EC2 environment and connect RDS using Node.js application. Know how to use Git to pull source code and configure .env file Know how to restore database from Snapshot Know how to use IAM role to ensure security for applications running on EC2 Successfully deploy 3 applications on Lightsail (WordPress, PrestaShop, Akaunting) Configure DB, static IP, SSH security, and proficiently operate Lightsail interface Understand and apply Lightsail Container, Docker, AWS CLI Successfully build, push, and deploy container image Access public endpoint to verify content displays correctly Create and configure S3 bucket for static website storage, enable versioning and manage access permissions Integrate CloudFront for fast distribution, secure bucket, website displays correctly and loads fast Get familiar with Amazon CloudWatch for system monitoring Collect and analyze metrics Know how to create alarms to receive alerts when system encounters issues "
},
{
	"uri": "https://tai-isme.github.io/workshop-template/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Complete the following labs:\nSet up Hybrid DNS with Route 53 Resolver Command line operations with AWS CLI NoSQL database with Amazon DynamoDB In-memory caching with Amazon ElastiCache Workshop on networking on AWS Content distribution with Amazon CloudFront Edge computing with CloudFront and Lambda@Edge Windows applications on AWS Directory services with AWS Managed Microsoft AD Build highly available web applications Virtual machine migration with AWS VM Import/Export Tasks to be completed this week: Day Tasks Start Date End Date Resources 2 - Deploy hybrid DNS architecture - Hands-on + Create AWS Managed Microsoft Active Directory using AWS Directory Service to simulate on-prem DNS system + Create Outbound Endpoint (forward DNS queries to internal DNS) + Create Resolver Rules + Create Inbound Endpoint (allow internal DNS to send DNS queries to Route 53) - Get familiar with AWS CLI - Hands-on + Install, configure AWS CLI + Practice using CLI commands with AWS services - Learn and master concepts, structure, capabilities of DynamoDB along with operations via AWS CLI, Console, CloudShell and Python SDK - Hands-on + CRUD + Query + Scan data + Use index 22/09/2025 22/09/2025 https://000010.awsstudygroup.com/vi/https://000011.awsstudygroup.com/vi/https://000060.awsstudygroup.com/vi/ 3 - NoSQL database with Amazon DynamoDB - In-memory caching with Amazon ElastiCache\n- Workshop on networking on AWS 23/09/2025 23/09/2025 https://000060.awsstudygroup.com/vi/https://000061.awsstudygroup.com/vi/https://000092.awsstudygroup.com/vi/ 4 - Content distribution with Amazon CloudFront\n- Edge computing with CloudFront and Lambda@Edge\n- Windows applications on AWS 24/09/2025 24/09/2025 https://000094.awsstudygroup.com/vi/https://000130.awsstudygroup.com/vi/https://000093.awsstudygroup.com/ 5 - Directory services with AWS Managed Microsoft AD\n- Build highly available web applications 25/09/2025 25/09/2025 https://000095.awsstudygroup.com/https://000101.awsstudygroup.com/vi/ 6 - Virtual machine migration with AWS VM Import/Export\n26/09/2025 Week 3 Results: Understand how Route 53 works in an integrated environment Master how to use tools such as Outbound/Inbound Endpoints and Resolver Rules to control DNS query flow Know how to use aws configure to set up CLI Know how to use CLI scripts to aggregate and manage resource usage Understand the concepts and structure of DynamoDB Know how to use CLI, console to operate with DynamoDB Master the concepts of cluster, node, shard and how Redis works Distinguish between cluster-mode-enabled and cluster-mode-disabled Clearly understand the AWS VPC model, main components and how the following work:\n- Subnet, Route Table, Internet Gateway, NAT Gateway\n- Security Group vs NACL\n- Elastic Network Interface (ENI) and Elastic IP (EIP) Distinguish network connection types:\n- VPC Peering: 1-1 connection between VPCs (does not support transit routing)\n- Transit Gateway (TGW): connect multiple VPCs in hub-and-spoke model\n- VPN Site-to-Site: establish hybrid network between AWS and on-prem\n- AWS Direct Connect: dedicated connection, low latency to AWS Protect and accelerate S3 with CloudFront Can independently configure complete CloudFront Distribution Practice using VM Import/Export service to import VM from on-premises into EC2 and vice versa "
},
{
	"uri": "https://tai-isme.github.io/workshop-template/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Learn about optimization: operational excellence, security, reliability, performance, cost optimization Complete related labs Tasks to be completed this week: Day Tasks Start Date End Date Resources 2 - Lab 22: Automatically start/stop servers and send messages via Slack with AWS\n- Hands-on:\n+ Create EC2 instance\n+ Create lambda function, add webhook to slack, get webhook url to receive notifications from lambda url\n- Lab 29: Create system monitoring dashboard with Amazon Cloudwatch and Grafana\n- Hands-on:\n+ Create EC2 instance\n+ Create IAM User to configure Grafana, create IAM role assigned to EC2 instance\n+ Install, configure Grafana to view CPUUtilization statistics of EC2 instance\n- Lab 27: Manage resources by group using Tag and Resource Groups\n- Hands-on:\n+ Add tags when launching resources\n+ Create resource group to group related resources\n- Lab 28: Manage EC2 service access by Tag through IAM\n- Hands-on:\n+ Create policies with permissions: List EC2, tag when creating EC2, tag existing EC2, create EC2, manage instance state\n+ Create role containing those policies\n+ Assign that role to user 29/09/2025 29/09/2025 https://000022.awsstudygroup.com/vi/\nhttps://000029.awsstudygroup.com/vi\nhttps://000027.awsstudygroup.com/vi/\nhttps://000028.awsstudygroup.com/vi/ 3 - Lab 30: Limit User Permissions with IAM Permission Boundary\n- Hands-on:\n+ Create policy to limit maximum permissions\n+ Create user then set policy boundary\n- Lab 44: Limit Role Switching by Condition\n- Hands-on: Create IAM users, user groups, configure IAM Role, limit access permissions for IAM User\n- Lab 18: Check and evaluate security standards with AWS Security Hub\n- Lab 33: Manage Keys with Key Management Service (AWS KMS)\n- Hands-on:\n+ Create Policies, IAM Users, User Groups for access control\n+ Create symmetric keys\n+ Create S3 Bucket, upload data to S3 and configure encryption with the created key for files\n+ Configure Cloud Trail to log S3 activity and Athena to query logs recorded directly in S3 30/09/2025 30/09/2025 https://000030.awsstudygroup.com/vi/ https://000044.awsstudygroup.com/vi/ https://000018.awsstudygroup.com/vi/ https://000033.awsstudygroup.com/vi/ 4 - Lab 13: Deploy system backup plan with AWS Backup\n- Hands-on:\n+ Create Backup Plan, configure schedule and resources to backup\n+ Set up SNS Notification to receive backup notifications\n- Lab 19: Link Virtual Private Clouds (VPC) with VPC Peering - Hands-on:\n+ Create 2 VPCs\n+ Create VPC Peering to establish connection between 2 VPCs\n+ Configure Route Table, add route to other VPC using Peering connection\n+ Configure Network ACL, block all allow only specific CIDR\n- Lab 20: Centrally manage connections with AWS Transit Gateway\n- Hands-on:\n+ Prepare 4 EC2 instances using CloudFormation\n+ Create Transit Gateway, use TGW Attachment to link VPCs, use TGW Route Table to determine paths 01/10/2025 01/10/2025 https://000013.awsstudygroup.com/vi/\nhttps://000019.awsstudygroup.com/vi/\nhttps://000020.awsstudygroup.com/vi/ 5 - Lab 15: Deploy Application with Docker\n- Hands-on:\n+ Deploy application locally\n+ Deploy with Docker Image\n+ Package each service as image\n+ Create container network to allow containers with images to communicate\n+ Deploy each container with images\n+ Deploy with Docker Compose\n+ Package services as images\n+ Configure .yml file, use docker compose to deploy containers configured in .yml file simultaneously\n+ Push images to ECR or Docker Hub for image storage\n- Review theory of services 02/10/2025 02/10/2025 https://000015.awsstudygroup.com/vi/\nhttps://www.youtube.com/playlist?list=PL4NoNM0L1m72HCTkOQUiIsHT8LRxdjeKJ 6 - Lab 31: Manage services and automate tasks using AWS System Manager\n- Hands-on:\n+ Create 2 windows instances as example\n+ Assign AmazonSSMManagedInstanceCore permission to both instances\n+ Update patches and security for both instances simultaneously using Patch manager\n+ Use Run command to send commands remotely\n- Lab 58: Work with AWS System Manager - Session Manager\n- Hands-on: + Assign AmazonSSMManageInstanceCore role to instance\n+ Connect to EC2 instance with internet access\n+ Connect to EC2 instance without internet access (through 3 endpoints: ssm, ssmmessages, ec2messages)\nUse S3 bucket to store session logs to view command history\n- Review theory of services\n03/10/2025 03/10/2025 https://www.youtube.com/playlist?list=PL4NoNM0L1m72HCTkOQUiIsHT8LRxdjeKJ\nhttps://000031.awsstudygroup.com/vi/\nhttps://000058.awsstudygroup.com/vi/ Week 4 Results: Know how to use Lambda service to automate starting/stopping EC2 instances and how to configure to receive notifications on Slack -\u0026gt; Goal is to reduce costs for instances that only operate in a short time period.\nKnow how to install, configure a dashboard with Grafana to monitor resources used on AWS.\nCreate tags when launching resources, filter resources by tags, add tags to resources using CLI\nUse resource group to group resources with related tags together\nAlways set IAM privileges at the minimum level, only grant necessary permissions when using.\nUse Patch manager to automatically update operating system patches or automatically update security\nUse Run command to send commands remotely to multiple instances simultaneously\nUnderstand the concept, how Session manager works, its advantages and why should use Session manager to manage servers instead of traditional methods.\nUnderstand the difference between attaching policy directly and using permissions boundary\nUnderstand more about IAM operations, requests to services, authenticate those requests and how assumeRole works\nMaster security standards and get familiar with AWS Security Hub console interface\nUnderstand the concept of symmetric and asymmetric keys in KMS service, how to use to encrypt data uploaded to S3 Bucket\nKnow how to use Cloud Trail to log S3 activities and use Athena to query those logs.\nKnow how to configure AWS Backup to protect data, automate backup and recovery processes\nConfigure SNS Notification to receive notifications related to backups\nUnderstand VPC Peering concepts, Network ACL, Cross-Peering DNS\nKnow how to configure ACL and create VPC Peering to connect 2 VPCs\nKnow how to configure Transit Gateway to simplify connecting multiple VPCs compared to VPC Peering\nGet familiar with operations to deploy an application using AWS with Docker\n"
},
{
	"uri": "https://tai-isme.github.io/workshop-template/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Complete serverless labs in Application Modernization on AWS Tasks to be completed this week: Day Tasks Start Date End Date Resources 2 - Lab 78: Serverless - Lambda interacting with S3 and DynamoDB\n- Lab 79: Serverless - Guide to writing Frontend calling API Gateway 06/10/2025 06/10/2025 https://000078.awsstudygroup.com/vi\nhttps://000079.awsstudygroup.com/vi 3 - Lab 80: Serverless - Deploy application on SAM\n- Lab 81: Serverless - Authentication with Amazon Cognito 07/10/2025 07/10/2025 https://000080.awsstudygroup.com/vi\nhttps://000081.awsstudygroup.com/vi 4 - Lab 82: Serverless - Set up static website with SSL on S3\n- Lab 83: Serverless - Handle orders with SQS and SNS 08/10/2025 08/10/2025 https://000082.awsstudygroup.com/vi\nhttps://000083.awsstudygroup.com/vi 5 - Lab 84: Serverless - CI/CD with AWS CodePipeline\n- Lab 85: Serverless - Monitor Lambda with CloudWatch and X-Ray 09/10/2025 09/10/2025 https://000084.awsstudygroup.com/vi\nhttps://000085.awsstudygroup.com/vi 6 - Lab 86: Serverless - Introduction to AWS AppSync 10/10/2025 10/10/2025 https://00006.awsstudygroup.com/vi Week 5 Results: Completed serverless labs to understand implementations for project 1 "
},
{
	"uri": "https://tai-isme.github.io/workshop-template/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Prepare proposal for project 1 Participate in knowledge sharing workshops Tasks to be completed this week: Day Tasks Start Date End Date Resources 2 - Learn about suitable services for project 1, estimate costs 11/08/2025 13/08/2025 https://tai-isme.github.io/workshop-template/vi/2-proposal/ 3 - Design architecture of services used in project 1 12/08/2025 13/08/2025 https://tai-isme.github.io/workshop-template/vi/2-proposal/ 4 - Design architecture of services used in project 1 13/08/2025 13/08/2025 https://cloudjourney.awsstudygroup.com/ 5 - Participate in Workshop \u0026ldquo;DATA SCIENCE ON AWS\u0026quot;\n- Write recap for DATA SCIENCE ON AWS workshop\n- Participate in workshop \u0026ldquo;Reinventing DevSecOps with AWS Generative AI\u0026rdquo;\u0026rdquo; - Write recap for \u0026ldquo;Reinventing DevSecOps with AWS Generative AI workshop 16/10/2025 16/10/2025 https://tai-isme.github.io/workshop-template/vi/4-eventparticipated/4.2-event2/ https://tai-isme.github.io/workshop-template/vi/4-eventparticipated/4.1-event1/ 6 - Revise architecture after receiving feedback from team\n- Points to be revised:\n+ Place CloudFront + S3 outside region + Draw detail diagram for each flow + Number each step\n+ Description for each step + Review arrow directions (when to use 2-way vs 1-way)\n+ Place Secret Manager and Guard Duty in same corner\n+ Place CloudTrail near CloudWatch for better monitoring appearance\n17/10/2025 17/10/2025 https://tai-isme.github.io/workshop-template/vi/2-proposal/ Week 6 Results: Consider costs when using services for the project. All team members understand the architecture for the upcoming project. Have a clear view of system components and structure. Understand how data is stored and transferred in the system. After \u0026ldquo;DATA SCIENCE ON AWS\u0026rdquo; workshop:\nMaster basic concepts and distinguish AI/ML technology layers. Understand the purpose of each AWS service suitable for tasks such as natural language processing, image recognition, speech conversion, text extraction, recommendation. Master the process of deploying a typical ML pipeline (from raw data to model deployment). After \u0026ldquo;Reinventing DevSecOps with AWS Generative AI\u0026rdquo; workshop:\nMaster the security integration flow from planning stage to monitoring. How to use Amazon Q to accelerate detection, analysis, and response to vulnerabilities. Know suitable tools for each phase (SAST/DAST, dependency/IaC scanning, monitoring). How to optimize costs and operations on cloud using auto scale and AWS supporting tools. "
},
{
	"uri": "https://tai-isme.github.io/workshop-template/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Learn the 4 main content groups for mid-term exam Tasks to be completed this week: Day Tasks Start Date End Date Resources 2 - Design secure architecture\n+ IAM, MFA, SCP, Encryption (KMS, TLS/ACM)\n+ Security Groups, NACLs, GuardDuty, Shield, WAF, Secret Manager 18/10/2025 22/10/2025 IAM, SCP:Viet-AWS, AWS IAM Basic, Docs, LAB 2\nMFA: LAB 1\nKMS:Viet-AWS, TLS: F5 DevCentral, ACM: AWS\nSG, NACL: Viet-AWS, AWSStudyGroup, GuarDuty: Lab 98\nShield, WAF: AWS\nSecrets Manager: AWS\nhttps://artempolynko.com/wp-content/uploads/2025/05/AWS-Solutions-Architect-Associate-Questions.pdf 3 - Design flexible and resilient architecture (Resilient Architectures)\n+ Multi-AZ, Multi-Region, DR Strategies, Auto Scaling\n+ Route 53, Load Balancing, Backup \u0026amp; Restore\n- Revise 8 content items in proposal, recalculate costs (increase API Gateway, reduce DynamoDB) 18/10/2025 22/10/2025 https://www.youtube.com/watch?v=1_AR-Me9us4 https://artempolynko.com/wp-content/uploads/2025/05/AWS-Solutions-Architect-Associate-Questions.pdf 4 - Security (Secure Architectures)\n- Flexibility and Resilience (Resilient Architectures)\n- High Performance (High-Performing Architectures)\n- Cost Optimization (Cost-Optimized Architectures) 18/10/2025 22/10/2025 https://artempolynko.com/wp-content/uploads/2025/05/AWS-Solutions-Architect-Associate-Questions.pdf/\nhttps://d1.awsstatic.com/training-and-certification/docs/AWS_Certified_Solutions_Architect_Associate_Sample_Questions.pdf\nhttps://www.whizlabs.com/blog/aws-solutions-architect-associate-exam-questions/\nhttps://easy-prep.org/aws-solutions-architect-associate-exam-questions/full-length-practice-test 5 - Learn SAM CLI\n- Learn structure and how to write template.yaml file 23/10/2025 24/10/2025 https://youtu.be/MipjLaTp5nA?si=cyGcu1VEBGHhkUi4\nhttps://youtu.be/W81ssQiBhcY?si=87QsQ6TicPSelkxo\nhttps://youtu.be/mhdX4znMd2Q?si=k6V_Hig8MDKdnqkq 6 - Learn SAM CLI\n- Learn structure and how to write template.yaml file\n- Participate in WORKSHOP \u0026ldquo;CLOUD INFRASTRUCTURE \u0026amp; OS ON AWS\u0026rdquo; 23/10/2025 24/10/2025 https://youtu.be/NDVJKz_MyFk?si=70pXCRDP7uadbYic\nhttps://youtu.be/gsE3bEPv0S0?si=AQbAz2_dK54doHhY Week 7 Results: Learn Solution Architecture knowledge to prepare for mid-term exam. Learn more about Nitro System technologies, concepts, and knowledge about Container \u0026amp; Orchestration of AWS through workshop sessions "
},
{
	"uri": "https://tai-isme.github.io/workshop-template/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Study for mid-term exam Tasks to be completed this week: Day Tasks Start Date End Date Resources 2 - Study via SAA-C03 app 27/10/2025 27/10/2025 https://docs.google.com/document/d/1BYGBu2KjeQgARkxTBP7dsnKu66KVhBxOv-GkmmjSOw8/edit?usp=sharing 3 - Study via SAA-C03 app 28/10/2025 28/10/2025 https://docs.google.com/document/d/1BYGBu2KjeQgARkxTBP7dsnKu66KVhBxOv-GkmmjSOw8/edit?usp=sharing 4 - Study via SAA-C03 app, solve exam questions (1-5) 29/10/2025 29/10/2025 https://docs.google.com/document/d/1BYGBu2KjeQgARkxTBP7dsnKu66KVhBxOv-GkmmjSOw8/edit?usp=sharing 5 - Study via SAA-C03 app, solve exam questions (6-10) 30/10/2025 30/10/2025 https://docs.google.com/document/d/1BYGBu2KjeQgARkxTBP7dsnKu66KVhBxOv-GkmmjSOw8/edit?usp=sharing 6 - Take exam 31/10/2025 31/10/2025 Week 8 Results: Exam result: 27/65 "
},
{
	"uri": "https://tai-isme.github.io/workshop-template/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Deploy Identity Service with Amazon Cognito Tasks to be completed this week: Day Tasks Start Date End Date Resources 2 - Learn how to start a serverless project on AWS - Research microservices architecture with Lambda - Learn about AWS SAM CLI and how to use it 03/11/2025 03/11/2025 https://youtu.be/gde38Yk5PQI?si=8uGMEhk23kzhN9GY\nhttps://youtu.be/J0aEfUUervE?si=xoF6gtLUIoBRwr3r\nhttps://youtu.be/KoY6fS77pDc?si=2vjMKXpGB6obHeKp\nhttps://youtu.be/0sgXYYJoVrA?si=sdrCQctWZAsN7jSH 3 - Learn how to code Lambda functions with Java 17 - Research AWS SDK for Java v2 - Study best practices for serverless applications 04/11/2025 04/11/2025 https://youtu.be/9kJ83XIMa_8?si=XjWbHRWPagtKyw_D\nhttps://youtu.be/frnveY8CU3M?si=W4sQSflqgS30lb6k\nhttps://youtu.be/2m1yraouNyg?si=ntR5hsre_aozyuTT 4 - Initialize project structure with SAM CLI - Implement Identity Service: + Create Lambda handlers for authentication + Configure Cognito User Pool + Set up API Gateway endpoints - Create template.yaml and samconfig.toml files 05/11/2025 05/11/2025 5 - Fully implement handlers for authentication flow: + RegisterHandler, LoginHandler, LogoutHandler + VerifyEmailHandler, ResendVerificationCodeHandler + ForgotPasswordHandler, ResetPasswordHandler + ChangePasswordHandler - Create CognitoClient to interact with Cognito User Pool - Fix endpoint errors when testing with LocalStack 06/11/2025 06/11/2025 6 - Implement handlers for user profile and admin features: + GetUserProfileHandler, UpdateUserProfileHandler + AdminInviteHandler, RedeemInviteHandler + InviteEmailSenderHandler with SNS + RefreshTokenHandler - Create DynamoDB tables: UsersTable, InvitesTable - Implement Cognito triggers: PreSignUp, PostConfirmation, PreTokenGeneration - Debug and fix endpoint issues 07/11/2025 07/11/2025 Week 9 Results: Implemented Identity Service with 17 Lambda handlers: authentication flow (register, login, logout, email verification), password management (forgot, reset, change), user profile, admin invitation system, token refresh, Cognito activation.\nIntegrated Amazon Cognito User Pool, API Gateway authorization, DynamoDB tables, and SNS for email notifications.\nBuilt a multi-module project structure.\n"
},
{
	"uri": "https://tai-isme.github.io/workshop-template/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Introduction This worklog records the journey of a 12-week internship at FCJ (from 08/08/2025 to 28/11/2025), focusing on learning and implementing services on Amazon Web Services (AWS). Throughout this process, I progressed from basic AWS knowledge to deploying a serverless system with a microservices architecture.\nWeekly Content Week 1: Getting familiar with AWS and basic services (IAM, VPC, EC2)\nWeek 2: Implementing Lab exercises for backend deployment with RDS, Lightsail, S3, and CloudWatch\nWeek 3: Learning advanced services through Lab exercises: Route 53, DynamoDB, ElastiCache, CloudFront\nWeek 4: Implementing Lab exercises related to optimization: Security, Reliability, Performance, and Cost Optimization\nWeek 5: Serverless Architecture with Lambda, API Gateway, and SAM\nWeek 6: Preparing Proposal for Project 1 and attending Workshop sessions\nWeek 7: Learning Solution Architecture and preparing for midterm exam\nWeek 8: Reviewing and taking the SAA-C03 midterm exam\nWeek 9: Deploying Identity Service with Amazon Cognito\nWeek 10: Deploying Identity Service and implementing Academic Service\nWeek 11: Setting up CI/CD Pipeline and CloudWatch Monitoring\nWeek 12: Testing and completing Workshop Documentation\n"
},
{
	"uri": "https://tai-isme.github.io/workshop-template/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Recap \u0026ldquo;DATA SCIENCE ON AWS\u0026rdquo; Event Purpose Explore the journey of building a modern Data Science system.\nSpeakers List Văn Hoàng Kha – Cloud Solutions Architect, AWS Community Builder\nBạch Doãn Vương – Cloud DevOps Engineer, AWS Community Builder\nHighlights Introduction to AI and Concepts (Mr. Kha) Artificial Intelligence (AI): Acts as a virtual assistant; a technology/technique capable of learning like humans. It learns from available data and learns from new, previously unseen data at very high speeds (though not as fast as human thought). Machine Learning (ML): A method to implement AI; computers learn based on large amounts of simple, text-based data to make predictions. Deep Learning: Learning divided into multiple layers/levels to learn complex, diverse data from various fields, connected like a neural network in the human brain. Generative AI (GenAI): Learns from extremely large language/data models available only at major tech companies (e.g., Data collected by Google over decades via user search results -\u0026gt; birth of a Large Language Model (GenAI)). Generative AI is superior to ML and DL in that it can create content like composing poetry, generating images, and videos. (e.g., Gemini from Google, Bedrock from AWS, etc.) AI/ML on the AWS Platform (Mr. Kha) AWS has developed over 200 services and over 10,000 features. History: AWS uses the services it develops for its own systems before selling them to the market. After releasing to the market, they collect feedback and requests from customers to further develop services and features based on that feedback. AWS services are divided into 3 layers: Top Layer (AI Services): Software serving all current problems. Image processing, audio, text-to-speech (and vice versa), text extraction from documents. Services allow users to use APIs for self-development. Middle Layer (ML Services): For users who want to manage, prepare data, process data, train machine learning, and fine-tune models themselves. The training service available is Amazon SageMaker, similar to Google Colab. Bottom Layer (ML Frameworks and Infrastructure): AWS provides powerful hardware and infrastructure to train large models, partnering with providers like NVIDIA. AWS AI Services (Mr. Kha) Amazon Comprehend: Natural Language Processing using machine learning to find insights and relationships in text. Amazon Translate: Translation service using Deep Learning to provide more accurate and natural translations than traditional algorithms. Amazon Transcribe: Speech recognition to convert speech to text. Amazon Polly: Generates speech on demand, converts text to speech. Amazon Textract: Text recognition, using Machine Learning to extract text, handwriting, and data from scanned documents. Amazon Rekognition: Service for object and face detection, video analysis, content moderation for movies, sports scenarios (e.g., VAR checks in soccer). Amazon Personalize: Helps build product/movie recommendation systems based on user behavior, similar to how Netflix or YouTube recommend content. Machine Learning Model Building Process (Mr. Kha) Feature Engineering: The process of preparing data, cleaning data, and converting data into machine language so the machine can understand it.\nModel Training: Using prepared data to teach the machine.\nTuning and Evaluation: Checking model accuracy. If not accurate, must return to data preparation and processing.\nModel Deployment: Allowing end-users to try it out.\nMonitoring: After the trial process, collect user feedback to re-change/update data to increase model accuracy.\nTraining models using Amazon SageMaker Canvas: After training, deployment can be done automatically without writing code, just drag and drop.\nDemo Session (Mr. Vuong) Processing and cleaning data from the IMDb dataset with AWS Glue: Upload raw data to S3. Use AWS Glue to perform automated cleaning tasks. Cleaned data is saved back to S3, ready for model training on Amazon SageMaker. Cost and Performance Comparison: Cloud vs. On-Premise: Cloud helps reduce infrastructure investment costs and optimizes operating costs thanks to the pay-as-you-go model. Cloud allows for rapid resource scaling. Flexible and fast testing of various models without hardware limitations. "
},
{
	"uri": "https://tai-isme.github.io/workshop-template/5-workshop/5.2-prerequisites/",
	"title": "Preparation Steps",
	"tags": [],
	"description": "",
	"content": "Environment Requirements To complete this workshop, we need to prepare the following.\nAWS Account:\nAn AWS account (Free Tier is acceptable) An IAM user with permissions for the following services: AWS CloudFormation AWS Lambda Amazon API Gateway Amazon S3 Amazon DynamoDB Amazon Cognito CloudWatch Logs Create AWS Access Key (Assuming you already have an IAM User)\nSign in to AWS Find and select the IAM service Select Users in the left menu to view the list of IAM Users Find and select the user you previously created Go to the Security Credentials section Choose Create Access Key to generate a new access key Choose CLI and check Confirmation Keep the Access Key and Secret Key secure and do not share them with others. AWS CLI:\nInstallation guide\nCheck version\naws --version AWS SAM CLI:\nInstallation guide\nCheck version\nsam --version AWS CLI Configuration Configure credentials\naws configure Enter the Access Key and Secret Key:\nNote: Do not expose your Access Key/Secret Key\nClone Workshop Repo Clone from Git\ngit clone https://github.com/Tai-isme/fcj-workshop-s3-notifications cd fcj-workshop-s3-notifications Download ZIP\nfcj-workshop-s3-notifications.zip Project directory structure\n📦fcj-workshop-s3-notifications ┣ 📂excel-import-frontend ┃ ┣ 📂src ┃ ┣ 📜package.json ┃ ┗ 📜vite.config.js ┗ 📂excel-import-workshop ┃ ┣ 📂src ┃ ┣ 📜pom.xml ┃ ┗ 📜template.yaml "
},
{
	"uri": "https://tai-isme.github.io/workshop-template/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "Teaching Center Management System AWS Serverless Solution for Teaching Center Management Project Proposal Word template: proposal-template.docx\n1. Executive Summary The project focuses on deploying an LMS (Learning Management System) platform serving core training operations, equivalent in scope to systems like lms-hcmuni.fpt.edu.vn. Specifically, the scope includes 2 main parts: academic management and authentication/identification and authorization.\nThe goal is to provide essential LMS capabilities: managing courses and classes; class schedules; searching and enrollment using enrollKey; role-based dashboards (Admin, Teacher, Student); managing instructor/student profiles; managing course documents on S3 (creating folders, uploading, downloading with enrollment verification); and batch data import from Excel to quickly initialize academic data. The authentication/identification and authorization part ensures login/authentication with Amazon Cognito (supporting OAuth/Google), invitation/redemption of invitations, password reset/change, token refresh, user profiles, and role-based authorization mechanisms to protect academic resources.\nIn the future, the system can gradually expand to other modules (CRM, payments, HRM, etc.) and consider integrating AI/IoT when needed, but these are not within the scope of the current deployment.\n2. Problem Statement Current Issues\nIn the LMS context, many training units operate in silos with discrete steps: creating courses/classes, publishing schedules, enrolling students, managing documents, and accessing profiles — typically across multiple different tools. This leads to fragmented data, difficult access control, manual enrollment processes (prone to errors), and inconsistent user experience between instructors and students.\nAs the number of courses/classes increases, there\u0026rsquo;s a lack of unified role-based authorization mechanisms, reliable authentication (SSO/OAuth), and clear academic APIs to support role-based dashboards. The absence of batch data import channels also causes delays when initializing new semesters.\nSolution\nThe platform is deployed on AWS Serverless architecture, focusing on addressing two main pillars:\nAuthentication/Identification and Authorization: Amazon Cognito for authentication (email/password, Google OAuth), invitations (invite/redeem), password reset/change, token refresh, user profiles; role-based authorization (ADMIN/TEACHER/STUDENT) to protect APIs and academic resources. Academic Management: APIs for managing courses/classes, unified search, enrollment using enrollKey (activating PRE_ENROLLED → ACTIVE), role-based dashboards, instructor/student profiles, and course document management on S3 (creating folders, presigned upload/download with enrollment verification). Supports Excel import for quick data initialization and full rollback on errors. The access flow: CloudFront → S3 (static content) and API Gateway → Lambda → DynamoDB (academic/identity data) with CloudWatch/SNS/Secrets Manager for monitoring and security. CI/CD is implemented through GitLab Runner combined with AWS SAM CLI.\nBenefits and Return on Investment (ROI)\nAccelerated development and deployment: Automated CI/CD with GitLab Runner and CloudFormation reduces feature release time from days to hours. Optimized operational costs: Serverless architecture (Lambda, DynamoDB, API Gateway) charges only per request, saving 40–60% of costs compared to traditional EC2. Comprehensive security: Secrets Manager and Cognito combined with IAM provide robust protection for sensitive data and strict access control. High access performance: CloudFront CDN accelerates access and reduces latency by 50–70%. Flexible scalability: The system automatically scales according to traffic without manual intervention. Proactive monitoring: CloudWatch + SNS provides real-time alerts, helping the technical team respond promptly to incidents. 3. Solution Architecture Detailed Description User Request Flow\nUsers open a browser and access the application through a domain. Amazon CloudFront receives the request: Checks the cache to return static content (HTML/CSS/JS) from S3 if available. If content is not in cache, CloudFront fetches from S3 and returns it to the user with low latency. Users receive the frontend and interact with the UI, generating API requests. Authentication \u0026amp; API Handling\nWhen users log in: Amazon Cognito authenticates login information (username/password or OAuth). Cognito generates a JWT token and returns it to the client. The frontend sends API requests with the JWT token to API Gateway. API Gateway performs: Verifies the JWT token with Cognito. If the token is valid, the request is forwarded to the corresponding Lambda function. If the token is invalid, API Gateway returns a 401 Unauthorized error. Lambda Processing \u0026amp; Data Access\nAWS Lambda executes business logic: Manages students, attendance, course registration, updates results, schedules… When accessing sensitive information (API keys, DB passwords), Lambda calls AWS Secrets Manager. Lambda reads/writes data to Amazon DynamoDB: DynamoDB stores data in a NoSQL model, optimized for read/write, automatically scaling when traffic increases. Supports queries using primary key (PK) or secondary index (GSI). Lambda logs to CloudWatch Logs, including requests, errors, and execution metrics. Security \u0026amp; Access Control\nAmazon Cognito: authenticates users and manages sessions, supporting OAuth/Google Sign-In. IAM Roles \u0026amp; Policies: control access permissions between AWS services (Lambda, DynamoDB, S3). API Gateway Authorization: validates JWT tokens from Cognito before allowing Lambda access. Secrets Manager: protects sensitive information (API keys, database credentials), enabling secure Lambda access. Monitoring \u0026amp; Alerts\nCloudWatch Logs collects logs from Lambda and API Gateway. Metrics \u0026amp; Alarms: Creates metrics from logs (CPU, error rate, latency, request count). Configures CloudWatch Alarms to trigger alerts when thresholds are exceeded. Amazon SNS sends real-time alerts to the operations team via email or HTTP/SMS endpoints. Combines CloudTrail + CloudWatch for auditing API actions and overall security. CI/CD \u0026amp; Deployment\nGitLab: stores source code and manages version control. GitLab Runner: Automatically triggers on code push or merge request. Runs CI/CD pipeline with stages: test, build, deploy. Installs dependencies and runs unit tests. Builds artifacts (ZIP package for Lambda). AWS SAM CLI / CloudFormation: GitLab Runner uses AWS SAM CLI for deployment. Deploys or updates the entire AWS infrastructure (API Gateway, Lambda, DynamoDB, S3, IAM Role). Ensures infrastructure follows IaC model, consistent across Dev/Prod environments. Automated CI/CD reduces errors and shortens deployment time. Summary\nRequest Path: User → CloudFront → API Gateway → Lambda (via Cognito Auth) → DynamoDB → Lambda → API Gateway → CloudFront → User. Security Path: Cognito Authentication → API Gateway Authorization → IAM Policies → Secrets Manager. Monitoring: CloudWatch Logs \u0026amp; Metrics → Alarms → SNS. CI/CD Path: GitLab → GitLab Runner → AWS SAM CLI → CloudFormation → AWS Resources. AWS Services Used Services Description Frontend \u0026amp; CDN CloudFront, S3 Content distribution, static storage Backend \u0026amp; Logic API Gateway, Lambda, DynamoDB, Secrets Manager, Cognito Serverless logic, data, authentication Monitoring CloudWatch Logs, CloudWatch Alarms, CloudWatch Metrics, SNS Monitoring, alerts, metrics collection CI/CD \u0026amp; IaC CloudFormation, SAM Automated deployment and infrastructure management (with GitLab Runner) 4. Technical Implementation Deployment Stages\nDevelopment Stage Complete business logic and main flow for Lambda functions. Write template.yaml file describing resources: API Gateway, Lambda Functions, DynamoDB, Cognito. Use AWS SAM CLI to deploy code and template.yaml to LocalStack for local testing. Deployment Stage: Use AWS SAM CLI to deploy code and template.yaml to the real AWS environment. Configure GitLab CI/CD with GitLab Runner to automate the build and deployment process. Technical Requirements\nHave an AWS account using Free Tier to deploy and use resources normally. The template.yaml file must be correctly configured to fully describe all services. The system must have an automatic rollback mechanism in case of deployment failure. 5. Roadmap \u0026amp; Deployment Milestones Pre-Internship (Week 0): Learn AWS services to prepare for the project. Survey, analyze requirements and related departments of real centers (HR, Training, Admissions). Internship (Week 1-12): Week 1–3: System design, UI design, overall architecture, and prepare documentation (proposal, diagrams, SAM template). Week 4–8: Develop core modules (student management, instructor management, class management, user authentication). Local testing using LocalStack. Week 9–11: Integrate modules, finalize CI/CD pipeline, deploy the system to the real AWS environment. Week 12: End-to-end testing, evaluate results, complete report, and propose future development directions. Post-Internship (Expansion Phase – Week 12 onwards): Upgrade the system, optimize performance, and integrate AI (personalized learning analytics) and IoT (smart classroom management). 6. Budget Estimation You can view costs on AWS Pricing Calculator\nOr download the budget estimate files pdf | csv | json\nInfrastructure Costs\nS3 Standard: 0.32 USD/month (10 GB, 5,000 PUT requests, 100,000 GET requests). CloudFront: 1.33 USD/month (10 GB, Data transfer out to origin 0.1 GB, Number of HTTPS requests 100,000). Amazon API Gateway: 0.38 USD/month (300,000 requests). AWS Lambda Function - Include Free Tier: 0.00 USD/month (400,000 requests, 512 MB storage). Amazon DynamoDB: 0.62 USD/month (Data storage 2 GB, 50,000 Writes, 200,000 Reads) Amazon Cognito Lite Tier: 0.00 USD/month (500 MAUs) Amazon CloudWatch: 2.10 USD/month (3 custom metrics, 1 GB logs, 1 dashboard, 2 alarms) AWS Secrets Manager: 0.40 USD/month (1 secret) Amazon SNS: 0.00 USD/month (1M requests, 1M Lambda deliveries) AWS CloudFormation: 0.00 USD/month GitLab Runner: 0.00 USD/month (self-hosted or GitLab Free Tier) Total: 5.15 USD/month, 61.80 USD/12 months\n7. Risk Assessment Risk Matrix\nAWS configuration errors (IAM, Lambda, API Gateway, Cognito): High impact, medium probability Exceeding AWS Free Tier limits: Medium impact, low probability. Data loss on S3/DynamoDB: High impact, low probability. Integration errors between AWS services: Medium impact, low probability. External attacks (SQL injection, XSS, unauthorized access): High impact, low to medium probability Mitigation Strategies\nAWS Configuration: Carefully check template.yaml, deploy on LocalStack before production deployment. Free Tier Exceeding: Monitor costs regularly, set up Billing Alerts, optimize resources. Data Loss: Enable S3 Versioning, periodically backup DynamoDB data. Service Integration Errors: Ensure services operate in the same Region, verify IAM Roles and cross-service access permissions. Application Security: Validate input at Lambda layer, use Cognito for strict authentication, configure CORS properly on API Gateway, apply principle of least privilege for IAM roles, enable CloudWatch and API Gateway logging for auditing. Contingency Plan\nDeployment failures: Rollback using AWS SAM CLI or restore previous Lambda version through CloudFormation stack. Budget overrun: Pause non-essential services, optimize architecture and resource usage. Security incidents: Review CloudWatch logs, disable compromised users/tokens via Cognito, review IAM permissions, isolate affected Lambda functions. 8. Expected Outcomes The teaching center management system is successfully deployed on AWS Serverless, ensuring stable, secure, and scalable operations. Optimize operational costs by leveraging AWS Free Tier and serverless architecture, reducing initial infrastructure investment. Ensure high access performance, fast response time, and flexible scalability. Guarantee data safety with backup, versioning, and strict access control mechanisms. CI/CD integration automates deployment, testing, and rollback, ensuring efficient and reliable development processes. "
},
{
	"uri": "https://tai-isme.github.io/workshop-template/5-workshop/5.4-deploy-backend/5.4.2-sam-deploy/",
	"title": "SAM Deploy",
	"tags": [],
	"description": "",
	"content": "Deploy with AWS SAM SAM CLI will package the code and deploy the entire infrastructure to AWS through CloudFormation.\nDeploy There are 2 ways to deploy the application using SAM:\nMethod 1 — Create S3 Bucket Manually aws s3 mb s3://demo-workshop-be-\u0026lt;YOUR-ID-ACCOUNT\u0026gt; --region ap-southeast-1 sam deploy --s3-bucket \u0026lt;bucket-name-just-created\u0026gt; --stack-name \u0026lt;stack-name\u0026gt; --region ap-southeast-1 Method 2 — Use --guided (recommended for first time) sam deploy --guided To keep it simple and avoid creating a bucket manually, we will use Method 2. --guided will ask for parameters (stack name, region, permissions, etc.) and save the configuration to samconfig.toml so next time you only need to run sam deploy.\nSAM will ask some guided questions When running sam deploy --guided, SAM will ask for some parameters to configure (for example):\nStack Name [excel-import-workshop]: Enter the stack name (or leave blank to use the current project name). AWS Region [ap-southeast-1]: Enter Region (or leave blank to use ap-southeast-1). Parameter Environment [dev]: Enter environment (default is dev if left blank). Confirm changes before deploy [y/N]: Enter y to review changes before deploying. Allow SAM CLI IAM role creation [y/N]: Enter y to allow SAM to create IAM roles for Lambda. Disable rollback [y/N]: Enter n to enable rollback if deployment fails (recommended n). Save arguments to configuration file [y/N]: Enter y to save the configuration to samconfig.toml (so next time you only need to run sam deploy). If you choose y, SAM will continue asking for the configuration file name (default is samconfig.toml) and environment:\nSAM configuration file [samconfig.toml]: samconfig.toml SAM configuration environment [default]: default In this example I chose n to not save the configuration.\nDeployment Process Preparing CloudFormation\nCloudFormation Change Set\nSAM will display the list of resources to be created:\nConfirm deploy: Enter: y to deploy\nCloudFormation Execution\nCloudFormation Execution Success\nIf you have deployed once with --guided and chose y at save configuration file: Then the next deployment only needs:\nsam build sam deploy SAM will read the config from samconfig.toml.\nVerify Created Resources Check CloudFormation Stack Open CloudFormation Console Find stack excel-import-workshop Stack status should be: CREATE_COMPLETE Check Lambda Open Lambda Console Select Functions Check API Gateway Open API Gateway Console Select api Check S3 Bucket Open S3 Console Select Bucket workshop-excel-imports Check User pool Open Cognito Console Select ExcelWorkshopUsers All resources have been deployed successfully and completely.\n"
},
{
	"uri": "https://tai-isme.github.io/workshop-template/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Deploy Identity Service to AWS Deploy Academic Service with Teacher and Student Management Set up monitoring infrastructure with CloudWatch and SNS Tasks to be completed this week: Day Tasks Start Date End Date Resources 2 - Fix bugs and deploy Identity Service to AWS\n- Check resources on AWS Console: Lambda, API Gateway, DynamoDB, Cognito 10/11/2025 10/11/2025 3 - Test Identity Service\n- Set up CloudWatch Logs for all Lambda functions - Create monitoring-alarms.yaml for Identity Service 11/11/2025 11/11/2025 4 - Deploy Academic Service\n- Create template-academic.yaml with basic resources - Set up shared module for common utilities 12/11/2025 12/11/2025 5 - Implement CRUD Teacher Management\n- Create TeacherRepository and TeachersTable 13/11/2025 13/11/2025 6 - Implement Student Management 14/11/2025 14/11/2025 Week 10 Results: Completed and successfully deployed Identity Service with 17 Lambda handlers: Authentication: Register, Login, Logout, Verify Email, Resend Verification Password: Forgot, Reset, Change Password Profile: Get, Update User Profile Admin: Invite users, Send email via SNS, Redeem invites Token: Refresh token mechanism Triggers: PreSignUp, PostConfirmation, PreTokenGeneration Fully implemented JWT token generation and validation "
},
{
	"uri": "https://tai-isme.github.io/workshop-template/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Add Import Data feature from Excel for Academic Service Deploy Academic Service to AWS Set up a complete CI/CD pipeline with GitLab Runner Configure CloudWatch monitoring and SNS alerting for the entire system Complete Infrastructure as Code with AWS SAM Tasks to be completed this week: Day Tasks Start Date End Date Resources 2 - Design and implement Import Data feature: + Create S3 bucket for uploading Excel files + Implement GenerateImportUploadUrlHandler (generate presigned URL) + Implement ImportS3TriggerHandler (trigger on new file) + Use Apache POI library to parse Excel - Create ImportJobsTable to track import status - Design import data schema for Teachers and Students 17/11/2025 17/11/2025 3 - Complete Import Data handlers: + Implement GetImportJobStatusHandler (check status) + Implement ListImportJobsHandler (view history) + Parse Excel and validate data format 18/11/2025 18/11/2025 4 - Deploy Academic Service to AWS\n- Test Teacher/Student CRUD functions\n- Test Import Data flow 19/11/2025 19/11/2025 5 - Set up CI/CD pipeline with GitLab: + Create .gitlab-ci.yml file with stages: build, test, deploy + Configure GitLab OIDC to authenticate with AWS + Set up GitLab Runner (using Docker image) + Jobs: build:be:identity, build:be:academic + Jobs: test:be:identity, test:be:academic + Jobs: deploy:be:identity:dev, deploy:be:academic:dev - Configure artifacts and caching - Commit code to trigger CI/CD pipeline 20/11/2025 20/11/2025 6 - Set up CloudWatch monitoring: + Create monitoring-alarms.yaml for Identity and Academic services + Configure alarms for Lambda + Configure alarms for API Gateway\n+ Configure alarms for DynamoDB\n- Set up SNS topic and subscribe to receive emails: tcm-pipeline-notifications 21/11/2025 21/11/2025 Week 11 Results: Successfully set up CI/CD pipeline with GitLab:\n.gitlab-ci.yml file with clear workflow rules and stages GitLab OIDC authentication with AWS (no Access Keys required) Parallel jobs for Identity and Academic services Build stage: Maven clean install, SAM build, SAM package Test stage: JUnit tests with Mockito, coverage reports Deploy stage: SAM deploy with samconfig.toml Stored artifacts to speed up builds (Maven .m2 repository) Environment-specific deployments: dev, staging, prod Manual approval for production deployments Comprehensive monitoring and alerting with CloudWatch:\nCreated monitoring-alarms.yaml for both Identity and Academic services Lambda alerts: Errors \u0026gt; 5%, Execution time \u0026gt; p99, Throttles, Concurrent executions API Gateway alerts: Client errors 4XX, Server errors 5XX, Latency DynamoDB alerts: Read/Write throttles, Consumed capacity SNS topic: tcm-pipeline-notifications for sending alerts Subscribed emails for real-time notifications CloudWatch dashboard for visualizing metrics (can be added later) 30-day retention policy to optimize costs "
},
{
	"uri": "https://tai-isme.github.io/workshop-template/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Test and debug the entire system (Identity + Academic Services) Write complete Workshop documentation Tasks to be completed this week: Day Tasks Start Date End Date Resources 2 - End-to-end testing of the entire Identity Service\n- Fix bugs discovered during testing 24/11/2025 24/11/2025 3 - End-to-end testing of the Academic Service\n- Fix bugs discovered during testing 25/11/2025 25/11/2025 4 - End-to-end testing of the CI/CD pipeline: + Push code changes and verify automatic build/test + Verify artifacts are cached correctly + Test parallel jobs for Identity and Academic\n- Fix bugs discovered during testing 26/11/2025 26/11/2025 5 - Write Workshop documentation 27/11/2025 27/11/2025 6 28/11/2025 28/11/2025 Week 12 Results: "
},
{
	"uri": "https://tai-isme.github.io/workshop-template/5-workshop/5.3-architecture/",
	"title": "Architecture",
	"tags": [],
	"description": "",
	"content": "System Architecture Overview The Excel Import system is designed as a Serverless Event-Driven Architecture, leveraging AWS managed services to reduce operational overhead and optimize costs.\nDetails Endpoints:\nPOST /register (RegisterFunction) POST /confirm (ConfirmFunction) POST /login (LoginFunction) POST /logout (LogoutFunction) POST /upload-url (GenerateUploadUrlFunction) GET /import/jobs (ListImportJobsFunction) GET /jobs/{jobId} (GetJobStatusFunction) Core Processing Function\nProcess Flow:\nGet file from S3 (event.Records[0].s3) Parse Excel using Apache POI Validate each row (email format, required fields) Batch write to DynamoDB (25 items/batch) Update ImportJob status \u0026amp; statistics Output: Updated ImportJob record Tables: StudentsTable, CoursesTable, ImportJobsTable Amazon S3 (File Storage): Storage for user-imported files. "
},
{
	"uri": "https://tai-isme.github.io/workshop-template/4-eventparticipated/4.3-event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": "Recap \u0026ldquo;[GAPV] Introduction to AI/ML GenAI with AWS\u0026rdquo; Event Purpose Share core concepts regarding AI, Machine Learning, Deep Learning, and LLMs. Share the role of AWS in supporting and optimizing these technologies. Speakers List Phạm Nguyễn Hải Anh - Cloud Engineer\nHighlights Concepts ML, Deep Learning, and LLMs are shared in an evolutionary context, with newer versions migrating/evolving from previous ones. AWS manufactures its own Graviton Processors spanning four generations; newer generations offer higher performance, which implies higher costs. Graviton processors are prioritized for inference processes. Suitable use cases for Graviton Processors: Text generation with models up to 70 billion parameters. Data preparation and processing. Retrieval Augmented Generation (RAG) techniques. Deep Learning, GPU, and factors affecting cost and time: Instance cost per time unit, training time (dependent on the model\u0026rsquo;s parameter count). Number of samples in the dataset. Compute power of the resources used. Challenges in inference: Latency in text generation, text transmission speed. Cost to host instances. AWS collaborates with NVIDIA to create P5, P4 instances for training and G6, G5 for inference. AWS AI Chips manufactured by AWS, such as Inferentia 2 and Trainium 2, serve both training and inference. AWS AI Services Amazon Rekognition (Image/Video Processing)\nPain point: Expensive investment, complex processing, rapid development speed required, and scarcity of ML resources. Solution: Provides simpler management via no-code/low-code, rapid build via SDK/API, high-quality results without AI experts, and allows for easy customization. Key services: Content moderation: Manage content, detect negative content. Face Liveness: Anti-spoofing. Face detection and analysis: Identify gender, age, and emotions on faces. Face search: Identify faces in large datasets (up to 20 billion faces). ID extraction: Extract information from profiles and documents. Amazon SageMaker Canvas (No-Code/Low-Code)\nPain point: ML teams lack human resources, require extensive coding and technical skills, and find it hard to update tools for rapid development. Solution: Provides pre-trained models and allows customization with own data without needing to code. Generative AI (Amazon Bedrock)\nPain point: Lack of Foundation Models trained specifically for a task, need to customize Foundation Models with internal enterprise data (requiring privacy), and difficulties in managing infrastructure and costs. Solution: Provides APIs to access famous Foundation Models globally (Claude, Stable Diffusion, etc.), allows customizing Foundation Models with the enterprise\u0026rsquo;s own data. RAG technique helps improve the accuracy and quality of the Foundation Model\u0026rsquo;s content, avoiding hallucinations where the model provides false information. Challenges in building RAG: Managing multiple data sources, choosing suitable vector types to convert documents to digital data, and efforts in building models/systems. Bedrock Knowledge Base: Fully managed by AWS, allows secure access to Foundation Models and Agents, and provides easy access to relevant data. Bedrock Security using Guardrails: Helps filter and protect sensitive data (PII). Defines behaviors to block (e.g., prevents generation of violent or sensitive content). Limits generated content to avoid hallucinations. New models on Bedrock: Nova Canvas (generates 2k x 2k resolution images). Nova Reel (generates video from text and images). Key Takeaways Gained a comprehensive view of how AWS supports businesses in leveraging the power of AI/ML and GenAI. LLM is a form of Deep Learning. Deep Learning is a subset of Machine Learning. Machine Learning is a part of Artificial Intelligence (AI). "
},
{
	"uri": "https://tai-isme.github.io/workshop-template/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1 - Optimizing HPC Workflows with Auto-Scaling Clusters in Ansys Gateway Powered by AWS This blog introduces Ansys Gateway powered by AWS, an HPC solution on AWS that enables automatic scaling of computing resources based on engineering simulation demand. The article presents the architecture, process for creating and managing HPC clusters, integration of storage services such as EFS, FSx for Lustre/OpenZFS, and the use of AWS ParallelCluster with Slurm for automatic node provisioning and deprovisioning. Users can optimize costs, improve simulation efficiency with flexible workflows, suitable for large Ansys applications requiring high performance.\nBlog 2 - Modernizing Snowflake Corporate\u0026rsquo;s Kubernetes Infrastructure with Bottlerocket and Karpenter This blog describes Snowflake Corporate\u0026rsquo;s Kubernetes infrastructure modernization journey by transitioning from Amazon Linux 2 to Bottlerocket – a container-optimized operating system – and using Karpenter to automate node scaling. The migration process was executed in phases to ensure service continuity, delivering benefits in security, performance (accelerated node startup, reduced pod readiness time), reduced operational overhead, and cost savings. The article summarizes practical lessons and recommendations for enterprises operating large-scale EKS.\nBlog 3 - Seamless Migration: Secure Transition of Large IoT Device Fleets to AWS This blog describes a strategy and specific steps for large-scale migration of IoT systems to AWS IoT Core. The blog focuses on challenges when using self-managed brokers, such as high costs, scalability difficulties, security concerns, and slow innovation. AWS IoT Core provides numerous features supporting smooth transition without requiring device updates, multi-protocol compatibility, strong authentication, and large-scale scalability. The migration process comprises phases: preparation, backend transition, device migration, and cleanup, helping reduce risk and ensure system continuity when moving to a modern cloud platform.\nBlog 4 - \u0026hellip; Blog 5 - \u0026hellip; Blog 6 - \u0026hellip; "
},
{
	"uri": "https://tai-isme.github.io/workshop-template/5-workshop/5.4-deploy-backend/",
	"title": "Deploy Backend",
	"tags": [],
	"description": "",
	"content": "Deploy Backend with AWS SAM In this section, we will use a SAM Template to deploy the entire infrastructure to AWS.\nSteps Build with Maven (compile Java)\nBuild with SAM (package Lambda)\nDeploy with SAM (create CloudFormation stack)\nVerify Resources (check on the AWS Console)\n"
},
{
	"uri": "https://tai-isme.github.io/workshop-template/4-eventparticipated/4.4-event4/",
	"title": "Event 4",
	"tags": [],
	"description": "",
	"content": "Recap: “CLOUD INFRASTRUCTURE \u0026amp; OS ON AWS” Workshop Event Purpose Share perspectives, viewpoints, and knowledge about AWS infrastructure. Share and introduce Containers \u0026amp; Orchestration. Speakers List Phạm Nguyễn Hải Anh - Cloud Engineer Danh Hoàng Hiếu Nghị - GenAI Engineer\nHighlights Comparison between On-Premise and AWS Cloud Infrastructure On-Premise:\nHypervisor Installation: Typically requires manual installation by experts, labor-intensive. Infrastructure: The virtualization layer is tightly coupled with physical hardware; requires direct intervention by technicians for maintenance. Scale: Limited processing capability; cannot scale arbitrarily (e.g., CPU capacity remains fixed even with low user traffic). Cost: Requires upfront capital investment (CAPEX). AWS Cloud (EC2):\nHypervisor Installation: Pre-configured by AWS, managed via the Console and AWS interface, allowing direct interaction. Infrastructure: Virtualization challenges are solved via the Nitro System (detailed below). Scale: Ability to scale according to user traffic volume—a key factor when choosing Cloud. Cost: Pay-as-you-go model; pay only for what you use. Choosing the Right Instance Consider factors: Series, generation, options, size (Depending on usage needs). Example: Choose CPU-optimized series for compute-intensive apps, or storage-optimized for database/cache systems. Three payment models for EC2: On-Demand: Pay for what you use. Savings Plans: Commit to usage for 1 or 3 years for price discounts (suitable when plans are clear). Spot Instances: Cheapest option, uses spare EC2 capacity, but can be reclaimed by AWS at any time (suitable for testing). Each account\u0026hellip; Sharing on Nitro System Technology – A virtualization technology on AWS Cloud powering EC2 infrastructure. History: Started around 2013, continuously developed and improved, introduced at events like AWS re:Invent. Architecture: Unlike traditional virtualization (Hardware -\u0026gt; Virtualization Layer -\u0026gt; Instance), Nitro attaches a Nitro layer directly to the hardware infrastructure. 3-Part Structure: Nitro Card: Hardware cards attached to chips to provide interfaces connecting to services (e.g., EBS, VPC). Nitro Security Chip: A custom-designed chip attached to the mainboard to provide security features. Nitro Hypervisor: A lightweight, simple virtualization layer with high infrastructure compatibility. Security: Data Center Operators have no direct access to Nitro hardware. All access must be authenticated via API and logged in the AWS system. No API has access to user data. Nitro creates a Security Pledge for users. AWS Graviton: A new AWS chip that increases performance, reduces costs, and saves energy. Key Takeaways No one has direct access to user infrastructure. 100% of access must go through APIs with logging. The Nitro System is developed and managed via DevSecOps processes. Microservices architecture can be applied to infrastructure. Containers \u0026amp; Orchestration Introduction to containers. "
},
{
	"uri": "https://tai-isme.github.io/workshop-template/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://tai-isme.github.io/workshop-template/5-workshop/5.5-deploy-frontend/",
	"title": "Deploy Frontend",
	"tags": [],
	"description": "",
	"content": "Update config.js File Step 1: Get API Gateway Endpoint\nOpen CloudFormation Console Find stack excel-import-workshop Select Output tab Copy value of ApiUrl Step 2: Change Configuration\nGo to /excel-import-frontend/ directory Go to /excel-import-frontend/src directory Open config.js file Replace APP_API_URL with the value of ApiUrl copied earlier Save it Step 3: Build Frontend and Create S3 Bucket to Host Frontend\nNavigate to the Frontend directory and run the following commands in order:\ncd ./excel-import-frontend/ npm install npm run build aws s3 mb s3://workshop-frontend-\u0026lt;ACCOUNT-ID\u0026gt; --region ap-southeast-1 After the build is complete, the dist folder will be created.\nStep 4: Host Frontend on S3 Bucket\nGo to S3 Bucket Console Select Bucket workshop-frontend-\u0026lt;ACCOUNT-ID\u0026gt; just created Open dist folder (created from npm run build command in Step 3), press CTRL + A to copy all files and folders. Drag all files and folders copied into the Upload section of the Bucket Click Upload, after upload success click Close Go to Permissions tab Click Edit Block public access (bucket settings) Uncheck Block public access Go to Object Ownership change to ACLs enabled then Save Select Object tab select all files and folders click Action and select Make Public and Close. Click select Index.html copy Object URL in the new tab to access the website. "
},
{
	"uri": "https://tai-isme.github.io/workshop-template/4-eventparticipated/4.5-event5/",
	"title": "Event 5",
	"tags": [],
	"description": "",
	"content": "“AWS Cloud Mastery Series #1” Recap Event Purpose Sharing knowledge about AI/ML/GenAI on AWS. Speakers List Đinh La Hoàng Anh Danh Hoàng Hiếu Nghị - GenAI Engineer\nHighlights 1. AWS AI/ML Services Overview Focuses on Pre-trained AI Services that are ready to use via API calls, requiring no deep machine learning knowledge.\nAmazon Rekognition (Computer Vision): Analyzes images/videos, facial recognition, and content moderation (e.g., blurring sensitive images). Supports Custom Labels to train the model for detecting specific objects. Amazon Translate: Real-time translation, supports natural phrasing and handles Emojis. Can integrate with S3 for batch text translation. Amazon Textract (OCR): Extracts text, handwriting, and data from scanned documents. Understands layouts and tables in administrative documents. Amazon Transcribe: Converts speech to text, identifies speakers, and provides automatic punctuation. Amazon Polly: Converts text to speech with natural intonation. Amazon Comprehend (NLP): Natural Language Processing: Sentiment analysis (positive/negative), keyword extraction, and sensitive information (PII) detection. Amazon Kendra: Intelligent enterprise search tool, understands natural query semantics (exact keyword match not required). Supports RAG (Retrieval-Augmented Generation). Note: Cost is relatively high. Amazon Lookout Family: Anomaly detection services for industrial equipment (Equipment) or product defects (Vision). Amazon Personalize: Recommendation System based on user behavior to increase engagement and sales. Supports automatic content generation for products. 2. Foundation Models \u0026amp; Bedrock Agents Shifting from traditional Chatbots to AI Agents capable of decision-making.\nChatbot Evolution: From simple if-else logic -\u0026gt; Q\u0026amp;A Chatbots -\u0026gt; AI Agents (capable of reasoning and task execution). Development Strategy: Do not train models from scratch unless necessary. Use frameworks like LangChain, LangGraph for fast building and scalability. Challenges in Production (Scale): Handling multiple concurrent users. Memory: Ability to remember context and user trends across sessions. Identity: Managing authentication and Agent access authorization. Tools: Managing API calls and correct tool execution. Solution - Amazon Bedrock Agent Core: Runtime: Execution environment, application wrapper. Memory: Built-in long-term contextual memory. Identity: Specific user authorization. Demo: Browser Use - Agent automatically manipulates the browser to find top comments on YouTube. Key Takeaways Maximize the use of Pre-trained AI services (Rekognition, Textract\u0026hellip;) for standard problems to save development time. For complex problems requiring reasoning and action, shift towards building AI Agents on the Amazon Bedrock platform. Focus on Scaling and Security (Identity, Guardrails) right from the Agent design phase. "
},
{
	"uri": "https://tai-isme.github.io/workshop-template/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Excel-to-DynamoDB on AWS using S3 Notifications Overview In this workshop, we will build a complete serverless application to import data from Excel files to DynamoDB using AWS services:\nAWS Lambda to handle backend logic Amazon S3 to store Excel files S3 Event Notifications to automatically trigger Lambda when new files arrive Amazon DynamoDB to store data from excel Amazon Cognito for user authentication API Gateway to expose REST APIs We will perform:\nDeploy serverless application using AWS SAM Configure S3 Events to trigger Lambda Parse Excel files in Lambda and save to DynamoDB Integrate Cognito authentication Monitor import job progress System Architecture This workshop implements an event-driven serverless architecture with the following components:\nAPI Layer:\nAPI Gateway with Cognito Authorizer REST endpoints for authentication and import operations Processing Layer:\nLambda functions handling business logic S3 Event Notifications trigger processing Apache POI library to parse Excel files Storage Layer:\nS3 bucket for file uploads DynamoDB tables for data from excel files and import jobs Contents Workshop Overview Environment Setup System Architecture Deploy Backend (AWS SAM) Deploy Frontend (React) Test Application Cleanup Resources "
},
{
	"uri": "https://tai-isme.github.io/workshop-template/5-workshop/5.6-test-application/",
	"title": "Test Application",
	"tags": [],
	"description": "",
	"content": "Test Application In this section we test the entire workflow: user registration, authentication, uploading sample files and monitoring import status.\nSign up and authentication On the login screen, click Sign up. Fill in the information and click Sign up to create an account. Enter the Code sent to your email and click Verify Email. After verification, log in with the Email and Password you just created. Download sample file Download the sample import file: import-template.xlsx Upload file and import Go to the file upload function, select the sample file you just downloaded and upload it. After uploading, click Upload \u0026amp; Import to start the import process. Monitor progress The file will be uploaded to S3 Bucket and trigger Lambda to import data. Import status: Processing → if no error, it will change to Completed. Processing → if there is an error, it will change to Failed and the system will automatically rollback. Verify data after import Check file on S3\nRun the following command to check the file you just uploaded:\naws s3 ls s3://workshop-excel-imports-\u0026lt;ACCOUNT-ID\u0026gt; --recursive Check table data\nGo to DynamoDB Console → Explore items, select each table to view the data after import. "
},
{
	"uri": "https://tai-isme.github.io/workshop-template/4-eventparticipated/4.6-event6/",
	"title": "Event 6",
	"tags": [],
	"description": "",
	"content": "\u0026ldquo;AWS Cloud Mastery Series #2\u0026rdquo; Report Purpose of the Event Sharing about DevOps on AWS Equipping knowledge about CI/CD, Infrastructure as Code, and Container Services Guiding practice on Monitoring \u0026amp; Observability on AWS Speaker List Mr. Tinh\nMr. Kha - Cloud Security Engineer\nKey Contents 1. DevOps Mindset \u0026amp; Culture Why is DevOps needed? Bridge between Development and Operations: DevOps is someone who understands both areas, helping to solve the \u0026ldquo;code works on my machine but not on the server\u0026rdquo; problem. Development Trend: From DevOps Engineer → Platform Engineer, focusing on building an Internal Developer Platform instead of just supporting individual teams. Core elements of DevOps Consistent build environments: Ensure development, staging, and production environments are the same. Automation Everything: Reduce human error, save time, and ensure consistent repeatable processes. Continuous Learning: Share knowledge and learn continuously within the team. Reduce Mean Time To Recover (MTTR): Detect and resolve errors faster by deploying continuously and having a monitoring system. 2. Learning Path for DevOps Engineer T-shaped Skills Deep dive into one area first: Choose either System/Linux or Development, learn deeply about one area before expanding. From System: Linux fundamentals → Docker → Kubernetes → Cloud Services From Development: Understand application lifecycle → CI/CD → Container → Infrastructure Important Advice Master one thing at a time: Don\u0026rsquo;t learn everything at once. Learn one thing deeply to truly understand it first. Document everything: Take notes on everything, write so that others can read and understand. Soft skills: Ability to express and communicate is very important because DevOps is a bridge between teams. Don\u0026rsquo;t compare with others: Compare yourself today with yesterday, everyone has a different learning speed. Practice is more important than theory Don\u0026rsquo;t just watch video tutorials: Must practice immediately, cannot learn without doing. Limit using AI to copy code: ChatGPT can help but you must understand the code, don\u0026rsquo;t copy blindly. Learn from errors: When encountering errors, must debug and understand the cause yourself, don\u0026rsquo;t just copy the error message into ChatGPT. 3. DevOps Metrics \u0026amp; Monitoring DORA Metrics Deployment Frequency: How often code is deployed to production Lead Time for Changes: Time from code commit to running in production Mean Time to Recovery (MTTR): Average time to restore service when a failure occurs Change Failure Rate: Percentage of deployments causing a failure in production Monitoring \u0026amp; Observability Cannot manage what is not measured: Must have a monitoring system to know how the application is running. CloudWatch, X-Ray, Dashboards: Monitor metrics, logs, and performance. Alerting: Set up alerts when there are issues to handle quickly. 4. CI/CD Pipeline Continuous Integration (CI) Source Control: Git strategies (GitFlow, Trunk-based), how to manage code Code Quality: Scan code before committing Review code (do not use AI to auto-approve) Test automation Build \u0026amp; Test: CodeBuild, testing pipelines Security: Scan dependencies, check for vulnerabilities (injection attacks even when using AI code) Continuous Delivery/Deployment (CD) Continuous Delivery: Requires approval before deploying to production Continuous Deployment: Fully automated deployment, no intervention needed Deployment Strategies: Blue/Green Deployment Canary Deployment (70/30 split to test) Rolling Updates CI/CD Best Practices Version Control for everything: Have a clear branch strategy Write clear commit messages Create Pull Requests and have someone review Do not push straight to main branch Testing at every stage: Unit test before committing Integration test after merging Never commit untested code Automation workflow: Scan security Build Test Review Deploy 5. Infrastructure as Code Terraform Declarative approach: Define infrastructure as code Important commands: terraform init - Initialize terraform plan - Preview changes (IMPORTANT - must check before applying) terraform apply - Deploy infrastructure AWS CloudFormation \u0026amp; CDK CloudFormation: Templates, stacks, drift detection CDK (Cloud Development Kit): Infrastructure as Code using programming languages (TypeScript, Python, etc.) CDK generates CloudFormation templates 6. Container Services Docker Fundamentals Not just building images: Must understand container interface, networking, storage Docker Compose: Manage multi-container applications Best practices: Do not commit node_modules or dependencies Use .dockerignore Multi-stage builds to reduce image size Amazon ECS \u0026amp; EKS ECS: Managed container service, easy to use EKS: Kubernetes on AWS, more complex but flexible ECR: Container registry of AWS Key Takeaways DevOps is not just tools: It is culture, workflow, and mindset. Automation is key: But must understand before automating. Learning never stops: Technology changes fast, must learn continuously. Deep dive from fundamentals: Linux/Networking first, then up to container/orchestration. CI/CD is core: Must be able to practice from source control to deployment. Monitoring \u0026amp; Security: Important as code, cannot be ignored. "
},
{
	"uri": "https://tai-isme.github.io/workshop-template/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at AWS from 08/09/2025 to 12/12/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment. I participated in building and deploying the Teaching Center Management System project on AWS Serverless platform, focusing on two main components: academic management — managing courses, classes, enrollment using enrollKey, managing documents on S3, importing Excel, and authentication/identification and authorization — Amazon Cognito, invite/redeem, Google SSO, profile management. The main work includes: designing the overall architecture and infrastructure service architecture; writing Infrastructure as Code templates using AWS SAM/CloudFormation; developing AWS Lambda functions for academic and authentication modules; integrating API Gateway, DynamoDB, S3, Cognito; configuring CloudFront, Route 53, WAF for frontend; establishing CI/CD pipeline with Gitlab Runner; monitoring and alerting using CloudWatch Logs/Alarms and SNS. Through this, I significantly improved my skills in cloud architecture design, serverless development, IaC, CI/CD, security and monitoring on AWS, technical documentation writing, teamwork and communication.\nRegarding work ethic, I always strived to complete tasks well, complied with regulations and actively communicated with team members.\nTo objectively reflect the internship process, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Absorbing new knowledge and learning quickly ☐ ✅ ☐ 3 Proactiveness Self-initiative, taking on tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ☐ ✅ ☐ 5 Discipline Adhering to schedules, regulations, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ☐ ✅ 7 Communication Presenting ideas and reporting work clearly ✅ ☐ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ☐ ☐ ✅ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ✅ ☐ ☐ 11 Contribution to project/team Work effectiveness, innovation initiatives, recognition from the team ☐ ✅ ☐ 12 Overall General evaluation of the entire internship period ☐ ✅ ☐ Needs Improvement Need to improve the way of connecting team members in work to ensure good quality output of the project Need to improve thinking skills in finding ideas, presenting ideas and planning Learn to communicate better in daily communication and at work "
},
{
	"uri": "https://tai-isme.github.io/workshop-template/5-workshop/5.7-cleanup/",
	"title": "Cleanup Resources",
	"tags": [],
	"description": "",
	"content": "1) Delete S3 Buckets Content\n# Delete all objects in the bucket (empty) aws s3 rm s3://workshop-excel-imports-\u0026lt;YOUR-ACCOUNT-ID\u0026gt; --recursive aws s3 rm s3://workshop-frontend-\u0026lt;YOUR-ACCOUNT-ID\u0026gt; --recursive 2) Delete S3 Buckets\nAfter the bucket is empty, delete the bucket using the command:\naws s3 rb s3://workshop-excel-imports-\u0026lt;YOUR-ACCOUNT-ID\u0026gt; aws s3 rb s3://workshop-frontend-\u0026lt;YOUR-ACCOUNT-ID\u0026gt; 3) Handle SamCliSourceBucket created by SAM CLI\nSamCliSourceBucket is a temporary bucket created by AWS SAM CLI to upload source code when running sam deploy. If this bucket has Bucket Versioning enabled, you must manually delete the versioned objects before deleting the bucket. Open AWS Console -\u0026gt; S3 -\u0026gt; select SamCliSourceBucket (example name: aws-sam-cli-managed-default-...). Go to Properties tab -\u0026gt; Bucket Versioning. If Versioning = Enabled, delete the object versions (or temporarily disable versioning and delete each version) then empty the bucket. After the bucket has no objects (all versions), select Delete bucket and confirm. 4) Delete CloudFormation stack (SAM delete)\ncd ./excel-import-workshop/ sam delete --stack-name excel-import-workshop --region us-east-1 --no-prompts "
},
{
	"uri": "https://tai-isme.github.io/workshop-template/4-eventparticipated/4.7-event7/",
	"title": "Event 7",
	"tags": [],
	"description": "",
	"content": "\u0026ldquo;AWS Cloud Mastery Series #3 - Security Pillar\u0026rdquo; Report Purpose of the Event Sharing about AWS Well-Architected Security Pillar Equipping knowledge about 5 security pillars: IAM, Detection, Infrastructure Protection, Data Protection, Incident Response Event Information Time: Friday, November 29, 2025 (8:30 AM – 12:00 PM - Morning Only) Location: AWS Vietnam Office Speaker List Mr. Hoang Anh Mr. Thinh, Mr. Dat, Mr. Duc Anh Mr. (Long) - Security Specialist from the USA\nKey Contents Role of Security Pillar in Well-Architected Security Pillar: One of the 6 pillars of the AWS Well-Architected Framework Goal: Protect data, systems, and assets on AWS Top threats in Vietnam: Exposed S3 Buckets with sensitive data Hardcoded credentials in code (pushed to GitHub) Long-term credentials not rotated Public-facing databases Lack of monitoring and detection Core Principles Least Privilege: Grant only the minimum necessary permissions, no more, no less Zero Trust: Do not trust anyone/anything by default, always verify Defense in Depth: Multiple layers of security, not relying on just one layer Shared Responsibility Model AWS is responsible for: Security OF the Cloud (infrastructure, hardware, network) Customer is responsible for: Security IN the Cloud (data, IAM, applications, OS, network config) Must understand clearly: AWS does not automatically protect everything, you must configure correctly Pillar 1 — Identity \u0026amp; Access Management (8:50 – 9:30 AM) IAM Fundamentals IAM = Identity and Access Management: Service for authentication and access management Components: Users: Individual users Roles: Roles assumed by services/users Policies: JSON documents defining permissions Groups: Groups of users for easier management Best Practices Delete Root Access Keys: Root account has full access, very dangerous if exposed Least Privilege: Grant only necessary permissions, do not use wildcard (*) if not needed Avoid long-term credentials: Use temporary credentials instead Do not hardcode credentials: Use IAM Roles, STS tokens, do not leave username/password in code IAM Identity Center (SSO) Single Sign-On: Log in once, use for multiple accounts/applications Benefits: Easy to manage users in multi-account environment Automatically rotate credentials (15 minutes - 36 hours) Integrate with external identity providers ⚠️ Important Note: When enabling Identity Center = enable AWS Organizations → will terminate all credits of new accounts (cannot be refunded) But: Free credits $200 are still lost, but promotional credits (found credit, activate credit) are not affected Service Control Policies (SCP) SCP vs IAM: IAM: Allow user/role to do something SCP: Limit (Boundary) maximum permissions in organization Example: A1 license allows driving 60km/h (IAM) Road sign only allows 40km/h (SCP) Result: Only allowed to drive 40km/h Use case: Limit regions (only ap-southeast-1, ap-southeast-2), prevent creating expensive resources Permission Boundaries Maximum permission limit that a user can grant to others Scenario: Senior dev can create IAM for intern, but maximum permission is only equal to senior\u0026rsquo;s permission Prevention: Intern cannot turn off monitoring, delete logs to hide errors MFA (Multi-Factor Authentication) TOTP (Time-based OTP): Google Authenticator, Microsoft Authenticator Pros: Free, easy to setup, sync between multiple devices Cons: Susceptible to phishing if entered into fake website FIDO2 (Fast Identity Online 2): YubiKey, physical security keys Pros: Better anti-phishing (only works with registered domain) Cons: Costly ($25-$90), if key is lost = account lost (unless backup exists) Credential Rotation Problem: Hardcode password in code → push to GitHub → bot scans in seconds → account hacked AWS Secrets Manager: Automatically rotate secrets according to schedule 4 rotation steps: Create new secret Set secret in database/service Test if new secret works Mark new secret as \u0026ldquo;current\u0026rdquo;, old secret as \u0026ldquo;previous\u0026rdquo; During rotation time, if hacker has old secret, they are still kicked out when rotation is done Pillar 2 — Detection CloudTrail Organization Level Organization Trail: Enable CloudTrail for the entire organization, not setup per account Benefits: Single pane of glass to monitor all activities Automatic deployment for new accounts Centralized logging Event Types Management Events: API calls, Console actions (CreateBucket, DeleteInstance) Data Events: Object-level operations (GetObject, PutObject in S3) Network Activity: VPC Flow Logs integration Insights Events: Detect unusual activity patterns Infrastructure as Code CloudFormation/Terraform: Deploy CloudTrail and detection rules automatically Version Control: Manage detection logic via Git Automatic Deployment: Deploy across the organization GuardDuty - Intelligent Threat Detection Current Challenges Don\u0026rsquo;t know under attack: Lack of visibility No context: Don\u0026rsquo;t know severity, don\u0026rsquo;t know how to handle Delay Response: Average takes 206 days to detect breach (according to industry stats) GuardDuty Solutions Machine Learning-based: Automatically detect threats without manual rules Data Sources: CloudTrail logs (API activities) VPC Flow Logs (network traffic) DNS logs Kubernetes audit logs (if using EKS) Threat Intelligence: AWS threat intelligence + 3rd party feeds Finding Types: Compromised credentials Unusual API calls Cryptocurrency mining Malware detection Unauthorized access attempts Real-time Automation EventBridge Integration: GuardDuty → EventBridge → Lambda → Actions Automatic Response: Block IP addresses Revoke IAM credentials Isolate EC2 instances Send SNS notifications Create tickets Timing Consideration GuardDuty findings: Can take 5-10 minutes to generate finding EventBridge: Trigger immediately (real-time) when finding exists Trade-off: Cannot be faster than 5-10 minutes with GuardDuty, this is a service limitation Alternative for real-time: CloudWatch Metrics + Anomaly Detection for network anomalies (no need to wait for GuardDuty) Security Hub - Centralized Security Management Aggregate findings from multiple sources Aggregates from: GuardDuty Inspector Macie IAM Access Analyzer Systems Manager Firewall Manager Single dashboard: View all security findings in one place Pillar 3 — Infrastructure Protection VPC Segmentation Private vs Public Subnets: Database/backend must be in private, only expose what is necessary Network isolation: Separate VPCs for different environments (dev/staging/prod) Security Groups vs NACLs Security Groups: Stateful (return traffic automatically allow) Instance level Allow rules only NACLs: Stateless (must define both inbound and outbound) Subnet level Allow and Deny rules WAF + Shield + Network Firewall WAF (Web Application Firewall): Protect layer 7 (HTTP/HTTPS) Shield: DDoS protection (Standard free, Advanced has cost) Network Firewall: Stateful inspection, IPS/IDS capabilities Workload Protection EC2: Security groups, IMDSv2, SSM Session Manager (no need for SSH keys) ECS/EKS: Pod security policies, network policies, secrets management Pillar 4 — Data Protection KMS (Key Management Service) AWS Managed Keys (AMK): AWS automatically manages, you don\u0026rsquo;t control Customer Managed Keys (CMK): You create, you control Important: If key is lost = data lost, cannot recover Support key rotation (automatic or manual) Key Rotation Automatic Rotation: Default 365 days (1 year) AWS automatically creates new key, re-encrypts data Transparent for applications Manual Rotation: More complex: must decrypt → delete old key → create new key → re-encrypt Use when granular control is needed Encryption at Rest S3: Default encryption, bucket policies can enforce EBS: Encryption when creating volume RDS/DynamoDB: Encryption option when creating database Nitro System: Hardware-based encryption for EC2 Certificate Management (ACM) AWS Certificate Manager: Free SSL/TLS certificates for ELB, CloudFront, API Gateway Automatic renewal (60 days before expire) with DNS validation Note: Must have own domain, does not work with AWS default domain Macie - Sensitive Data Discovery Machine Learning-based: Automatically detect sensitive data (credit cards, SSN, PII) S3 scanning: Scan buckets, generate findings regarding data exposure Timing: Takes 15-20 minutes for first scan Integration: Findings send to Security Hub Data Classification \u0026amp; Guardrails Guardrails: Policy enforcement - do not trust developers will remember to encrypt Example: Bucket policy deny PutObject if no encryption header Zero Trust principle: Enforce instead of recommend Demo: HTTP vs HTTPS with Wireshark HTTP: Traffic is plaintext Wireshark capture: Username and password clearly visible Hacker can \u0026ldquo;eavesdrop\u0026rdquo; and take credentials HTTPS: Traffic encrypted with TLS Wireshark capture: Only see encrypted gibberish Need private key to decrypt (which attacker doesn\u0026rsquo;t have) Pillar 5 — Incident Response Modern Security Mindset Context: Complex, highly automated attacks → Must use Automation Incident Definition: Not just hack/data leak, but also Downtime, unusual High Traffic Shared Responsibility: AWS secures \u0026ldquo;OF\u0026rdquo; the Cloud, you secure \u0026ldquo;IN\u0026rdquo; the Cloud Core Defense Measures Mandatory AWS Tools: Organizations, SCPs, CloudTrail, Config, GuardDuty, Security Hub Identity Management: Eliminate long-lived credentials → Use SSO, IAM Roles, STS tokens Data \u0026amp; Network Protection: S3 not public, Database/RDP/SSH no Public IP Infrastructure as Code: Do not click Console → Use Terraform/CloudFormation Double Gate: High risk changes need ≥2 approval steps (SCP + Policy + Manual approval) Incident Response Lifecycle Preparation:\nHave playbooks, runbooks ready Train team Setup automation beforehand Tools ready (CloudTrail, GuardDuty, Security Hub) Detection \u0026amp; Analysis:\nMonitor alerts from GuardDuty, Security Hub Analyze logs, identify scope Determine severity Containment:\nIsolate affected resources Revoke credentials Preserve evidence (snapshots, logs) Do not delete anything - needed for forensics Eradication:\nRemove threat/vulnerability Patch systems Restore to clean state Recovery \u0026amp; Lessons Learned:\nRestore normal operations Post-mortem analysis Update playbooks Improve detection Modern Threats \u0026amp; Reality Check Shared Responsibility: AWS protects cloud infrastructure, you must protect data and config Modern threats today: AI-powered attacks (faster, smarter) Supply chain attacks Zero-day exploits Insider threats Q\u0026amp;A Q: Is there a way to optimize GuardDuty finding time? A: 5-10 minutes is a service limitation. If real-time is needed, use CloudWatch Metrics + Anomaly Detection for network anomalies. Q: Different approaches for real-time detection? A: Three approaches: Anomalies: CloudWatch Metrics (instant) Security Issues: GuardDuty (5-10 min) Compliance: AWS Config (minutes to hours depending on rule) Key Takeaways Security is a shared responsibility: AWS + You, not AWS automatically doing everything Defense in Depth: Multiple layers, not relying on just one layer Zero Trust: Do not trust by default, always verify Automate everything: Manual does not scale, automation helps fast response "
},
{
	"uri": "https://tai-isme.github.io/workshop-template/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "Overall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help when I encounter difficulties, even outside working hours. The workspace is clean, comfortable, and helps me focus better. The current working environment is very good, creating an environment that helps everyone connect and develop.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don\u0026rsquo;t understand, and always encourages me to ask questions. The admin team supports procedures, documentation, and creates favorable conditions for me to work smoothly.\n3. Relevance of Work to Academic Major\nThe work I was assigned aligns well with the knowledge I learned at school, while also expanding into new areas I had never encountered before. As a result, I was able to reinforce my foundational knowledge while learning practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication and conduct in a corporate environment. The mentor also shared many real-world experiences that helped me better plan my career.\n5. Company Culture \u0026amp; Team Spirit\nCompany culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable, and always supports and provides suggestions for questions and concerns from one another.\n6. Internship Policies / Benefits\nThe company allows flexible working hours when necessary, as long as the required hours are met. Workshops are organized quite frequently to support the learning process and help gain new knowledge.\nAdditional Questions What did you find most satisfying during your internship? The most satisfying thing during the internship was the environment and the mentors. What do you think the company should improve for future interns? Currently, I don\u0026rsquo;t see any areas that need improvement. If recommending to a friend, would you suggest they intern here? Why or why not? Yes! If you are interested in Cloud, especially AWS, this is a very suitable place because it provides you with an environment, enthusiastic mentors, learning materials, and workshops with lots of knowledge. Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Currently, I don\u0026rsquo;t have any. Would you like to continue this program in the future? Yes, I would very much like to continue. Any other comments (free sharing): None. "
},
{
	"uri": "https://tai-isme.github.io/workshop-template/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://tai-isme.github.io/workshop-template/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]